apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d34e8ba1cd53e39e6e2c0db39fe100d1757b99800243c18a29ad765129b0a009
      cni.projectcalico.org/podIP: 10.1.249.61/32
      cni.projectcalico.org/podIPs: 10.1.249.61/32
    creationTimestamp: "2025-05-08T00:46:45Z"
    generateName: calico-kube-controllers-79949b87d-
    generation: 1
    labels:
      k8s-app: calico-kube-controllers
      pod-template-hash: 79949b87d
    name: calico-kube-controllers-79949b87d-ckxgz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-kube-controllers-79949b87d
      uid: 3b7cf545-be1c-47d4-b2b4-a3c35bc6552e
    resourceVersion: "748891"
    uid: b40b86ab-cc91-4c65-a2f4-71356c2506b3
  spec:
    containers:
    - env:
      - name: ENABLED_CONTROLLERS
        value: node
      - name: DATASTORE_TYPE
        value: kubernetes
      image: docker.io/calico/kube-controllers:v3.29.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /usr/bin/check-status
          - -l
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-kube-controllers
      readinessProbe:
        exec:
          command:
          - /usr/bin/check-status
          - -r
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mcm8s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: calico-kube-controllers
    serviceAccountName: calico-kube-controllers
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mcm8s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:47:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:47:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://44cec0bf4fcdeacb0fde4f931361dde2549401d2ceca8130f0af7d6f94b072db
      image: docker.io/calico/kube-controllers:v3.29.3
      imageID: docker.io/calico/kube-controllers@sha256:5516ab776c38525fcc985a3030b4f2fd472da68b4170601c23bf6887bfdce703
      lastState:
        terminated:
          containerID: containerd://c80b761d3e4b77d7d37f5fbbed636786825f7ca082a4c335c6e572deb7da189f
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:22Z"
      name: calico-kube-controllers
      ready: true
      resources: {}
      restartCount: 98
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:46Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mcm8s
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.61
    podIPs:
    - ip: 10.1.249.61
    qosClass: BestEffort
    startTime: "2025-05-08T00:47:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-05-08T00:46:44Z"
    generateName: calico-node-
    generation: 1
    labels:
      controller-revision-hash: 64cd8dd55f
      k8s-app: calico-node
      pod-template-generation: "1"
    name: calico-node-9g7rt
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: 68175b7f-5af5-479f-b7e5-fe15e1504fd2
    resourceVersion: "748873"
    uid: b06ecc94-b164-441a-b42d-d23dc85b3ec5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - leo-vostro
    containers:
    - env:
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: WAIT_FOR_DATASTORE
        value: "true"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_NETWORKING_BACKEND
        valueFrom:
          configMapKeyRef:
            key: calico_backend
            name: calico-config
      - name: CLUSTER_TYPE
        value: k8s,bgp
      - name: IP
        value: autodetect
      - name: IP_AUTODETECTION_METHOD
        value: first-found
      - name: CALICO_IPV4POOL_VXLAN
        value: Always
      - name: CALICO_IPV6POOL_VXLAN
        value: Never
      - name: FELIX_IPINIPMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: FELIX_VXLANMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: FELIX_WIREGUARDMTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: CALICO_IPV4POOL_CIDR
        value: 10.1.0.0/16
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_FEATUREDETECTOVERRIDE
        value: ChecksumOffloadBroken=true
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/node:v3.29.3
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/calico-node
            - -shutdown
      livenessProbe:
        exec:
          command:
          - /bin/calico-node
          - -felix-live
        failureThreshold: 6
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: calico-node
      readinessProbe:
        exec:
          command:
          - /bin/calico-node
          - -felix-ready
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        requests:
          cpu: 250m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nk4g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /opt/cni/bin/calico-ipam
      - -upgrade
      env:
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CALICO_NETWORKING_BACKEND
        valueFrom:
          configMapKeyRef:
            key: calico_backend
            name: calico-config
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/cni:v3.29.3
      imagePullPolicy: IfNotPresent
      name: upgrade-ipam
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/cni/networks
        name: host-local-net-dir
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nk4g
        readOnly: true
    - command:
      - /opt/cni/bin/install
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        valueFrom:
          configMapKeyRef:
            key: cni_network_config
            name: calico-config
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CNI_MTU
        valueFrom:
          configMapKeyRef:
            key: veth_mtu
            name: calico-config
      - name: SLEEP
        value: "false"
      - name: CNI_NET_DIR
        value: /var/snap/microk8s/current/args/cni-network
      envFrom:
      - configMapRef:
          name: kubernetes-services-endpoint
          optional: true
      image: docker.io/calico/cni:v3.29.3
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nk4g
        readOnly: true
    nodeName: leo-vostro
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: calico-node
    serviceAccountName: calico-node
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /var/snap/microk8s/current/var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/snap/microk8s/current/var/lib/calico
        type: ""
      name: var-lib-calico
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /sys/fs/
        type: DirectoryOrCreate
      name: sys-fs
    - hostPath:
        path: /var/snap/microk8s/current/opt/cni/bin
        type: DirectoryOrCreate
      name: cni-bin-dir
    - hostPath:
        path: /var/snap/microk8s/current/args/cni-network
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/snap/microk8s/common/var/log/calico/cni
        type: ""
      name: cni-log-dir
    - hostPath:
        path: /var/snap/microk8s/current/var/lib/cni/networks
        type: ""
      name: host-local-net-dir
    - hostPath:
        path: /var/snap/microk8s/current/var/run/nodeagent
        type: DirectoryOrCreate
      name: policysync
    - name: kube-api-access-9nk4g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:47:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:46:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 250m
      containerID: containerd://9de1d700f32f90cf950cec8392ad6dd923ae98eda7f624b58c2e64b176cf0769
      image: docker.io/calico/node:v3.29.3
      imageID: docker.io/calico/node@sha256:eed399f2a727cfc1f374ab5c9cda6123c207e794ed8dc66c7eb6d8db412669e1
      lastState:
        terminated:
          containerID: containerd://b5fbbbbd204f43e12df32685d1c8ab6cccabd765d1707cdb4a2bf22161381ef7
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:25Z"
      name: calico-node
      ready: true
      resources:
        requests:
          cpu: 250m
      restartCount: 30
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:30Z"
      volumeMounts:
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/nodeagent
        name: policysync
      - mountPath: /var/log/calico/cni
        name: cni-log-dir
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nk4g
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    initContainerStatuses:
    - containerID: containerd://d245f023da9754bec026d3ea76910b8e0e2dd6a4ff296a5a8b7b26b245056e83
      image: docker.io/calico/cni:v3.29.3
      imageID: docker.io/calico/cni@sha256:53f826d3f565a6635b4d58ea4fcfdc0e7ea418ffd4dbb495b4c801074e6eb99c
      lastState: {}
      name: upgrade-ipam
      ready: true
      resources: {}
      restartCount: 30
      started: false
      state:
        terminated:
          containerID: containerd://d245f023da9754bec026d3ea76910b8e0e2dd6a4ff296a5a8b7b26b245056e83
          exitCode: 0
          finishedAt: "2025-06-03T00:51:27Z"
          reason: Completed
          startedAt: "2025-06-03T00:51:27Z"
      volumeMounts:
      - mountPath: /var/lib/cni/networks
        name: host-local-net-dir
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nk4g
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://d645d5a5dcfe0df7f7e7f667459d6f18cf231f0c0610fd08d834ebdd69a6e6fd
      image: docker.io/calico/cni:v3.29.3
      imageID: docker.io/calico/cni@sha256:53f826d3f565a6635b4d58ea4fcfdc0e7ea418ffd4dbb495b4c801074e6eb99c
      lastState: {}
      name: install-cni
      ready: true
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d645d5a5dcfe0df7f7e7f667459d6f18cf231f0c0610fd08d834ebdd69a6e6fd
          exitCode: 0
          finishedAt: "2025-06-03T00:51:29Z"
          reason: Completed
          startedAt: "2025-06-03T00:51:28Z"
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9nk4g
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 192.168.1.11
    podIPs:
    - ip: 192.168.1.11
    qosClass: Burstable
    startTime: "2025-05-08T00:46:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: a1c26216beb283e0ebff61ab45918be422243e6a3c5844a9e0bee69966547572
      cni.projectcalico.org/podIP: ""
      cni.projectcalico.org/podIPs: ""
    creationTimestamp: "2025-06-03T02:00:00Z"
    generateName: coleta-du-29148600-
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
      batch.kubernetes.io/job-name: coleta-du-29148600
      controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
      job-name: coleta-du-29148600
    name: coleta-du-29148600-ghz4r
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: coleta-du-29148600
      uid: af8c484d-b282-4d24-be81-4f7b97e124ef
    resourceVersion: "758705"
    uid: 988ea4b2-3ace-49f2-896a-c338ddebcf5d
  spec:
    containers:
    - args:
      - |
        apk add --no-cache coreutils;
        du -sh /var/lib/docker/containers /var/lib/kubelet/pods /var/log /opt/applications || true
      command:
      - /bin/sh
      - -c
      image: alpine:latest
      imagePullPolicy: Always
      name: coleta-du
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7fgpr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-7fgpr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T02:00:18Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T02:00:00Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T02:00:17Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T02:00:17Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T02:00:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3f22b5d40a7cde88b16c287374dcc17d08ae8531ce2f5905bb827c0175e005c6
      image: docker.io/library/alpine:latest
      imageID: docker.io/library/alpine@sha256:8a1f59ffb675680d47db6337b49d22281a139e9d709335b492be023728e11715
      lastState: {}
      name: coleta-du
      ready: false
      resources: {}
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3f22b5d40a7cde88b16c287374dcc17d08ae8531ce2f5905bb827c0175e005c6
          exitCode: 0
          finishedAt: "2025-06-03T02:00:16Z"
          reason: Completed
          startedAt: "2025-06-03T02:00:14Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7fgpr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Succeeded
    podIP: 10.1.249.34
    podIPs:
    - ip: 10.1.249.34
    qosClass: BestEffort
    startTime: "2025-06-03T02:00:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 7a5b1a74d8198f55f8e209960c8d1b2c9a4132189869143ca2e3243d2878cf72
      cni.projectcalico.org/podIP: 10.1.249.31/32
      cni.projectcalico.org/podIPs: 10.1.249.31/32
      priorityClassName: system-cluster-critical
    creationTimestamp: "2025-05-29T01:11:33Z"
    generateName: coredns-ccd8f67bc-
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: ccd8f67bc
    name: coredns-ccd8f67bc-kbwtk
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-ccd8f67bc
      uid: 64cb646b-000e-48cf-b84e-49b5f251071e
    resourceVersion: "748765"
    uid: 8c24f281-7820-4768-8786-0fd897565412
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: coredns/coredns:1.12.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-chgpn
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-chgpn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-29T01:11:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-29T01:11:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 70Mi
      containerID: containerd://bcd5afecd168a4839afabffa37570fb1d70b8b94aff01d4d307952aeb87ba3fb
      image: docker.io/coredns/coredns:1.12.1
      imageID: docker.io/coredns/coredns@sha256:e8c262566636e6bc340ece6473b0eed193cad045384401529721ddbe6463d31c
      lastState:
        terminated:
          containerID: containerd://3dba56d1c5767651d058825414cec50fcb33df33ad8b979330ba11a6f47d54bf
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:20Z"
      name: coredns
      ready: true
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:33Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-chgpn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.31
    podIPs:
    - ip: 10.1.249.31
    qosClass: Burstable
    startTime: "2025-05-29T01:11:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 47abd88247dfdb9a91c42329f8c8bea574984e9bd33fc14f047c1a3902c6d013
      cni.projectcalico.org/podIP: 10.1.249.56/32
      cni.projectcalico.org/podIPs: 10.1.249.56/32
    creationTimestamp: "2025-05-29T01:11:41Z"
    generateName: hostpath-provisioner-c778b7559-
    generation: 1
    labels:
      k8s-app: hostpath-provisioner
      pod-template-hash: c778b7559
    name: hostpath-provisioner-c778b7559-hnk6d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hostpath-provisioner-c778b7559
      uid: 4c4e8190-a31f-49ba-9634-1c0d84ef850b
    resourceVersion: "748788"
    uid: 7281f5ee-77a0-49ca-b8d9-f2c20fcf1f73
  spec:
    containers:
    - env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: PV_DIR
        value: /var/snap/microk8s/common/default-storage
      - name: BUSYBOX_IMAGE
        value: busybox:1.28.4
      image: cdkbot/hostpath-provisioner:1.5.0
      imagePullPolicy: IfNotPresent
      name: hostpath-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/snap/microk8s/common/default-storage
        name: pv-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z5ttc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: microk8s-hostpath
    serviceAccountName: microk8s-hostpath
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/snap/microk8s/common/default-storage
        type: ""
      name: pv-volume
    - name: kube-api-access-z5ttc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-29T01:11:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-29T01:11:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://68c8a685ebe0cfe15ff5dac1e510af38826934dacdb3d8e37cfed6b5272fff34
      image: docker.io/cdkbot/hostpath-provisioner:1.5.0
      imageID: docker.io/cdkbot/hostpath-provisioner@sha256:ac51e50e32b70e47077fe90928a7fe4d3fc8dd49192db4932c2643c49729c2eb
      lastState:
        terminated:
          containerID: containerd://608f5a126c2e0a8f8350af099d872225b35dbb4ae1295563595615344ba50af6
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T20:26:54Z"
      name: hostpath-provisioner
      ready: true
      resources: {}
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:38Z"
      volumeMounts:
      - mountPath: /var/snap/microk8s/common/default-storage
        name: pv-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-z5ttc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.56
    podIPs:
    - ip: 10.1.249.56
    qosClass: BestEffort
    startTime: "2025-05-29T01:11:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: fea17a2d1323f74e700b50bfd3cb1af7617a512a892760d21efe39f8cf1c8d51
      cni.projectcalico.org/podIP: 10.1.249.21/32
      cni.projectcalico.org/podIPs: 10.1.249.21/32
    creationTimestamp: "2025-06-03T01:27:27Z"
    generateName: metrics-server-64fc948c75-
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 64fc948c75
    name: metrics-server-64fc948c75-k9jvn
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-64fc948c75
      uid: 9e17243f-86cc-4e14-a5a8-4bd9add8fa7d
    resourceVersion: "754179"
    uid: bfa1eb4c-8d4a-4183-b16d-005fcb3a55bf
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=4443
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --kubelet-insecure-tls
      image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 4443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cnwf8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    nodeSelector:
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-cnwf8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:27:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:27:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:27:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:27:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:27:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 100m
        memory: 200Mi
      containerID: containerd://61793ffded4bcc229ab7a8818aa839f8e32133f9fadebb02d6fd5eef1cbf6f9d
      image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
      imageID: registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9
      lastState: {}
      name: metrics-server
      ready: true
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-03T01:27:27Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cnwf8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.21
    podIPs:
    - ip: 10.1.249.21
    qosClass: Burstable
    startTime: "2025-06-03T01:27:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: a8b854d814c8dd5442534740ff7d331414649c93617460e3adc208763d196d43
      cni.projectcalico.org/podIP: 10.1.249.22/32
      cni.projectcalico.org/podIPs: 10.1.249.22/32
    creationTimestamp: "2025-06-01T02:06:01Z"
    generateName: local-path-provisioner-74f9666bc9-
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 74f9666bc9
    name: local-path-provisioner-74f9666bc9-2lzt4
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-74f9666bc9
      uid: 004e38c5-7926-451b-b708-0b06e128d3a8
    resourceVersion: "748840"
    uid: 5cd4f85b-1d5d-4071-bbec-641496c05f34
  spec:
    containers:
    - command:
      - local-path-provisioner
      - --debug
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CONFIG_MOUNT_PATH
        value: /etc/config/
      image: rancher/local-path-provisioner:v0.0.31
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wfc49
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-wfc49
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T02:06:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T02:06:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a0b1bebd1d7b7a311e2818a714700ad5097ccfd36cb2ec246180eb5750121a13
      image: docker.io/rancher/local-path-provisioner:v0.0.31
      imageID: docker.io/rancher/local-path-provisioner@sha256:80496fdeb307541007621959aa13aed41d31db9cd2dc4167c19833e0bfa3878c
      lastState:
        terminated:
          containerID: containerd://306a9aa89a0b4e5bf1fd7e1e7a1f464130d95d110c60c02a1ff436f97a7a67cb
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:18Z"
      name: local-path-provisioner
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:41Z"
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wfc49
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.22
    podIPs:
    - ip: 10.1.249.22
    qosClass: BestEffort
    startTime: "2025-06-01T02:06:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: d6ea28ae33fd1aebf2a4449dc99a1521e7d837046cda6b06d67f77eceab3f80f
      cni.projectcalico.org/podIP: 10.1.249.15/32
      cni.projectcalico.org/podIPs: 10.1.249.15/32
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-05-08T00:51:23Z"
    generateName: controller-bb5f47665-
    generation: 1
    labels:
      app: metallb
      component: controller
      pod-template-hash: bb5f47665
    name: controller-bb5f47665-4gnnv
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: controller-bb5f47665
      uid: 0f5d6733-981b-406a-813b-fd56785c0f49
    resourceVersion: "748872"
    uid: 7c61a747-60a9-499e-a209-96086a149650
  spec:
    containers:
    - args:
      - --port=7472
      - --log-level=info
      - --tls-min-version=VersionTLS12
      env:
      - name: METALLB_ML_SECRET_NAME
        value: memberlist
      - name: METALLB_DEPLOYMENT
        value: controller
      image: quay.io/metallb/controller:v0.14.9
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 7472
        name: monitoring
        protocol: TCP
      - containerPort: 9443
        name: webhook-server
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/k8s-webhook-server/serving-certs
        name: cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jdbw5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: controller
    serviceAccountName: controller
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: cert
      secret:
        defaultMode: 420
        secretName: metallb-webhook-cert
    - name: kube-api-access-jdbw5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:51:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:51:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://26994db2c64f651e4c81f5dcf9ec333ece3b218b022e13fd59b56a68e40e1dc3
      image: quay.io/metallb/controller:v0.14.9
      imageID: quay.io/metallb/controller@sha256:86261567e5ff03978893bf03ea865275283ad1e3f0f20dd342ed501b651fdf78
      lastState:
        terminated:
          containerID: containerd://fdcca866de410d8c9694d1e62205d7e791d7e01c693a8ab12ec7ceb4cf8f1796
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:24Z"
      name: controller
      ready: true
      resources: {}
      restartCount: 37
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:31Z"
      volumeMounts:
      - mountPath: /tmp/k8s-webhook-server/serving-certs
        name: cert
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jdbw5
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.15
    podIPs:
    - ip: 10.1.249.15
    qosClass: BestEffort
    startTime: "2025-05-08T00:51:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-05-08T00:51:23Z"
    generateName: speaker-
    generation: 1
    labels:
      app: metallb
      component: speaker
      controller-revision-hash: 779c7bb879
      pod-template-generation: "1"
    name: speaker-9fj24
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: speaker
      uid: 6843b657-7950-4adf-9c92-0a5760a77816
    resourceVersion: "748797"
    uid: 14095854-a43a-489c-b7fd-b91cb8c3f28a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - leo-vostro
    containers:
    - args:
      - --port=7472
      - --log-level=info
      env:
      - name: METALLB_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: METALLB_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: METALLB_HOST
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: METALLB_ML_BIND_ADDR
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: METALLB_ML_LABELS
        value: app=metallb,component=speaker
      - name: METALLB_ML_SECRET_KEY_PATH
        value: /etc/ml_secret_key
      image: quay.io/metallb/speaker:v0.14.9
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: speaker
      ports:
      - containerPort: 7472
        hostPort: 7472
        name: monitoring
        protocol: TCP
      - containerPort: 7946
        hostPort: 7946
        name: memberlist-tcp
        protocol: TCP
      - containerPort: 7946
        hostPort: 7946
        name: memberlist-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /metrics
          port: monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_RAW
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ml_secret_key
        name: memberlist
        readOnly: true
      - mountPath: /etc/metallb
        name: metallb-excludel2
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bjqk4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: leo-vostro
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: speaker
    serviceAccountName: speaker
    terminationGracePeriodSeconds: 2
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: memberlist
      secret:
        defaultMode: 420
        secretName: memberlist
    - configMap:
        defaultMode: 256
        name: metallb-excludel2
      name: metallb-excludel2
    - name: kube-api-access-bjqk4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:51:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-08T00:51:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9bdbeabf4a160400cf10227a4c381f342f5f95ab4e227e7c097abb70d1f21f11
      image: quay.io/metallb/speaker:v0.14.9
      imageID: quay.io/metallb/speaker@sha256:b09a1dfcf330938950b65115cd58f6989108c0c21d3c096040e7fe9a25a92993
      lastState:
        terminated:
          containerID: containerd://b9f5c2045ca28ba6438f19e9d2dbb09f55572ca19771dedf54c758fce02a44f1
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:15Z"
      name: speaker
      ready: true
      resources: {}
      restartCount: 99
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:27Z"
      volumeMounts:
      - mountPath: /etc/ml_secret_key
        name: memberlist
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/metallb
        name: metallb-excludel2
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bjqk4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 192.168.1.11
    podIPs:
    - ip: 192.168.1.11
    qosClass: BestEffort
    startTime: "2025-05-08T00:51:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5f835d1869a7604477a9da7f15532d101ed0c51751b3873892a53c092622fae9
      cni.projectcalico.org/podIP: 10.1.249.55/32
      cni.projectcalico.org/podIPs: 10.1.249.55/32
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2025-06-01T04:43:36Z"
    generateName: alertmanager-kube-prom-stack-kube-prome-alertmanager-
    generation: 1
    labels:
      alertmanager: kube-prom-stack-kube-prome-alertmanager
      app.kubernetes.io/instance: kube-prom-stack-kube-prome-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.28.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-kube-prom-stack-kube-prome-alertmanager-76b844b954
      statefulset.kubernetes.io/pod-name: alertmanager-kube-prom-stack-kube-prome-alertmanager-0
    name: alertmanager-kube-prom-stack-kube-prome-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-kube-prom-stack-kube-prome-alertmanager
      uid: 34cea21d-9880-4c74-8ae9-15a23ba2ee08
    resourceVersion: "748824"
    uid: 85d75faa-9639-4de8-b9a4-a1bc95abee31
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - kube-prom-stack-kube-prome-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://kube-prom-stack-kube-prome-alertmanager.monitoring:9093
      - --web.route-prefix=/
      - --cluster.label=monitoring/kube-prom-stack-kube-prome-alertmanager
      - --cluster.peer=alertmanager-kube-prom-stack-kube-prome-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.28.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-kube-prom-stack-kube-prome-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        subPath: cluster-tls-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzqf
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzqf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-kube-prom-stack-kube-prome-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzqf
        readOnly: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prom-stack-kube-prome-alertmanager
    serviceAccountName: kube-prom-stack-kube-prome-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prom-stack-kube-prome-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-kube-prom-stack-kube-prome-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prom-stack-kube-prome-alertmanager-web-config
    - name: cluster-tls-config
      secret:
        defaultMode: 420
        secretName: alertmanager-kube-prom-stack-kube-prome-alertmanager-cluster-tls-config
    - emptyDir: {}
      name: alertmanager-kube-prom-stack-kube-prome-alertmanager-db
    - name: kube-api-access-hbzqf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        memory: 200Mi
      containerID: containerd://bc29c88ea56c16a588f840bbf4a31cd1589f56bccf50a1f524560955ef493fa8
      image: quay.io/prometheus/alertmanager:v0.28.1
      imageID: quay.io/prometheus/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba
      lastState:
        terminated:
          containerID: containerd://d6d5ce609c672a0359800c1db0b7d265e022db9e69521198677b431a401c4891
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          message: |
            time=2025-06-01T19:46:25.846Z level=INFO source=main.go:191 msg="Starting Alertmanager" version="(version=0.28.1, branch=HEAD, revision=b2099eaa2c9ebc25edb26517cb9c732738e93910)"
            time=2025-06-01T19:46:25.847Z level=INFO source=main.go:192 msg="Build context" build_context="(go=go1.23.7, platform=linux/amd64, user=root@fa3ca569dfe4, date=20250307-15:05:18, tags=netgo)"
            time=2025-06-01T19:46:25.894Z level=INFO source=coordinator.go:112 msg="Loading configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
            time=2025-06-01T19:46:25.895Z level=INFO source=coordinator.go:125 msg="Completed loading of configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
            time=2025-06-01T19:46:25.902Z level=INFO source=tls_config.go:347 msg="Listening on" address=[::]:9093
            time=2025-06-01T19:46:25.902Z level=INFO source=tls_config.go:386 msg="TLS is disabled." http2=false address=[::]:9093
            time=2025-06-01T19:46:30.861Z level=INFO source=coordinator.go:112 msg="Loading configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
            time=2025-06-01T19:46:30.861Z level=INFO source=coordinator.go:125 msg="Completed loading of configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
          reason: Unknown
          startedAt: "2025-06-01T19:46:25Z"
      name: alertmanager
      ready: true
      resources:
        requests:
          memory: 200Mi
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:33Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /alertmanager
        name: alertmanager-kube-prom-stack-kube-prome-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzqf
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://7655490591f77fef6a95037e9ae7f2bd8be8ca8a479c8dde4bea0c1a81f425cf
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:710458fdccd42d56bb5b20b548fe9a2f832c12aa26256e80cda48a9edb0454ff
      lastState:
        terminated:
          containerID: containerd://8ca4cd2ef69265b729144e8902d615c6b38b1f903c1da6711acad22d9e82c54a
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          message: |
            ts=2025-06-01T19:46:25.858052269Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:148 msg="Starting prometheus-config-reloader" version="(version=0.82.2, branch=, revision=4d008d5)" build_context="(go=go1.24.3, platform=linux/amd64, user=, date=20250512-08:50:19, tags=unknown)"
            ts=2025-06-01T19:46:25.858415129Z level=info caller=/workspace/internal/goruntime/cpu.go:27 msg="Leaving GOMAXPROCS=8: CPU quota undefined"
            level=info ts=2025-06-01T19:46:25.859272292Z caller=reloader.go:282 msg="reloading via HTTP"
            ts=2025-06-01T19:46:25.859296119Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:202 msg="Starting web server for metrics" listen=:8080
            ts=2025-06-01T19:46:25.859716232Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:347 msg="Listening on" address=[::]:8080
            ts=2025-06-01T19:46:25.859844189Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:386 msg="TLS is disabled." http2=false address=[::]:8080
            level=error ts=2025-06-01T19:46:25.860588462Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9093/-/reload\": dial tcp 127.0.0.1:9093: connect: connection refused"
            level=info ts=2025-06-01T19:46:30.865643535Z caller=reloader.go:548 msg="Reload triggered" cfg_in=/etc/alertmanager/config/alertmanager.yaml.gz cfg_out=/etc/alertmanager/config_out/alertmanager.env.yaml cfg_dirs= watched_dirs=/etc/alertmanager/config
            level=info ts=2025-06-01T19:46:30.865718261Z caller=reloader.go:330 msg="started watching config file and directories for changes" cfg=/etc/alertmanager/config/alertmanager.yaml.gz cfgDirs= out=/etc/alertmanager/config_out/alertmanager.env.yaml dirs=/etc/alertmanager/config
          reason: Unknown
          startedAt: "2025-06-01T19:46:25Z"
      name: config-reloader
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:33Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzqf
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    initContainerStatuses:
    - containerID: containerd://15f095da900cb9486a6fd59671996cc3b2575f4496fb893912139973230a6121
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:710458fdccd42d56bb5b20b548fe9a2f832c12aa26256e80cda48a9edb0454ff
      lastState: {}
      name: init-config-reloader
      ready: true
      resources: {}
      restartCount: 2
      started: false
      state:
        terminated:
          containerID: containerd://15f095da900cb9486a6fd59671996cc3b2575f4496fb893912139973230a6121
          exitCode: 0
          finishedAt: "2025-06-03T00:51:32Z"
          reason: Completed
          startedAt: "2025-06-03T00:51:32Z"
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzqf
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.1.249.55
    podIPs:
    - ip: 10.1.249.55
    qosClass: Burstable
    startTime: "2025-06-01T04:43:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 238debadbd6fed461c4b2bf4d7416e00d36791485f010dbb793cb2cbd7d888ed
      cni.projectcalico.org/containerID: 21a8c38e0b8f426789d7ce01ab15ec6088cad00eeb5e19588ed1132100a81cb3
      cni.projectcalico.org/podIP: 10.1.249.27/32
      cni.projectcalico.org/podIPs: 10.1.249.27/32
    creationTimestamp: "2025-06-03T01:15:34Z"
    generateName: black-prometheus-blackbox-exporter-54cfcb5d87-
    generation: 1
    labels:
      app.kubernetes.io/instance: black
      app.kubernetes.io/name: prometheus-blackbox-exporter
      pod-template-hash: 54cfcb5d87
    name: black-prometheus-blackbox-exporter-54cfcb5d87-prpgc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: black-prometheus-blackbox-exporter-54cfcb5d87
      uid: 2d033ceb-d5d7-4de5-84d5-f1e4eb6b6e52
    resourceVersion: "760578"
    uid: 07c1559b-31dc-4433-818e-d70b39dc72f5
  spec:
    automountServiceAccountToken: false
    containers:
    - args:
      - --config.file=/config/blackbox.yaml
      image: quay.io/prometheus/blackbox-exporter:v0.26.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: blackbox-exporter
      ports:
      - containerPort: 9115
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: black-prometheus-blackbox-exporter
    serviceAccountName: black-prometheus-blackbox-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: black-prometheus-blackbox-exporter
      name: config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:15:42Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:15:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:15:34Z"
      message: 'containers with unready status: [blackbox-exporter]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:15:34Z"
      message: 'containers with unready status: [blackbox-exporter]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T01:15:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5a727b28c87b742cb3c22552bed91d03c3af2c2f32862aedbacbb528fc297a16
      image: quay.io/prometheus/blackbox-exporter:v0.26.0
      imageID: quay.io/prometheus/blackbox-exporter@sha256:92e05d5fe0df01d3980518dc42f07b778a8997048b57392ebc7f7391ebd7bb06
      lastState:
        terminated:
          containerID: containerd://5a727b28c87b742cb3c22552bed91d03c3af2c2f32862aedbacbb528fc297a16
          exitCode: 1
          finishedAt: "2025-06-03T02:12:44Z"
          reason: Error
          startedAt: "2025-06-03T02:12:44Z"
      name: blackbox-exporter
      ready: false
      resources: {}
      restartCount: 16
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=blackbox-exporter pod=black-prometheus-blackbox-exporter-54cfcb5d87-prpgc_monitoring(07c1559b-31dc-4433-818e-d70b39dc72f5)
          reason: CrashLoopBackOff
      volumeMounts:
      - mountPath: /config
        name: config
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.27
    podIPs:
    - ip: 10.1.249.27
    qosClass: BestEffort
    startTime: "2025-06-03T01:15:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
      cni.projectcalico.org/containerID: 9204845aa1cec198a5df6cb798446784864ef795e444964906609c075b9e82e6
      cni.projectcalico.org/podIP: 10.1.249.44/32
      cni.projectcalico.org/podIPs: 10.1.249.44/32
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2025-06-03T00:48:22Z"
    generateName: kube-prom-stack-grafana-64774f5954-
    generation: 4
    labels:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.0-security-01
      helm.sh/chart: grafana-9.2.1
      pod-template-hash: 64774f5954
    name: kube-prom-stack-grafana-64774f5954-54jxb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prom-stack-grafana-64774f5954
      uid: a867347b-76e6-41f9-ad9f-95d8fe12a7a9
    resourceVersion: "749436"
    uid: dcd3dcf0-4f05-42d8-96d7-f7537c55ced5
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prom-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prom-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.30.0
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpptw
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prom-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prom-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.30.0
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpptw
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: kube-prom-stack-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: kube-prom-stack-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:12.0.0-security-01
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      - containerPort: 6060
        name: profiling
        protocol: TCP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpptw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    ephemeralContainers:
    - image: busybox
      imagePullPolicy: Always
      name: debugger-xbbjq
      resources: {}
      stdin: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
    - image: busybox
      imagePullPolicy: Always
      name: debugger-5kc5f
      resources: {}
      stdin: true
      targetContainerName: grafana
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
    - image: busybox
      imagePullPolicy: Always
      name: debugger-qc7b6
      resources: {}
      stdin: true
      targetContainerName: grafana-sc-datasources
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: kube-prom-stack-grafana
    serviceAccountName: kube-prom-stack-grafana
    shareProcessNamespace: false
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-prom-stack-grafana
      name: config
    - name: storage
      persistentVolumeClaim:
        claimName: kube-prom-stack-grafana
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: kube-prom-stack-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-wpptw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:48:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:53:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:53:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:48:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://91aa5e2f95292202b16128e377d9379629a16dd311904b80a287087ddf8f86cd
      image: docker.io/grafana/grafana:12.0.0-security-01
      imageID: docker.io/grafana/grafana@sha256:6fc2fee54f8551a3ae452a2dd14fd524143026e46a0f06b0302c79117fe89fcf
      lastState: {}
      name: grafana
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:32Z"
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpptw
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://2d4ceb2913ed8c9285a19aa3a89fd2db302141d3b8788712a78a6cc8e2fe4b92
      image: quay.io/kiwigrid/k8s-sidecar:1.30.0
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:9a326271c439b6f9e174f3b48ed132bbff71c00592c7dbd072ccdc334445bde2
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:31Z"
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpptw
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://57ef9e3bc5ca7c0a43f03e20d5ad657055a39899f8066bdd651e52a6f5b809c1
      image: quay.io/kiwigrid/k8s-sidecar:1.30.0
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:9a326271c439b6f9e174f3b48ed132bbff71c00592c7dbd072ccdc334445bde2
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      resources: {}
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:32Z"
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wpptw
        readOnly: true
        recursiveReadOnly: Disabled
    ephemeralContainerStatuses:
    - containerID: containerd://b299448401511bc3af9693d303a539c57773664e19323686f4951c68e1cea876
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b
      lastState: {}
      name: debugger-5kc5f
      ready: false
      resources: {}
      restartCount: 0
      state:
        running:
          startedAt: "2025-06-03T00:54:44Z"
    - containerID: containerd://2d0acae87dc7fa7c842281ac65cae2f1c553f4bbd05af6a1bc77f13294780b8e
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b
      lastState: {}
      name: debugger-qc7b6
      ready: false
      resources: {}
      restartCount: 0
      state:
        running:
          startedAt: "2025-06-03T00:55:19Z"
    - containerID: containerd://10da9535759303834bd2cee67aeb6b435e245296f4615950350ac00151f7d285
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b
      lastState: {}
      name: debugger-xbbjq
      ready: false
      resources: {}
      restartCount: 0
      state:
        running:
          startedAt: "2025-06-03T00:51:34Z"
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.44
    podIPs:
    - ip: 10.1.249.44
    qosClass: BestEffort
    startTime: "2025-06-03T00:48:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 6fe97c067463d7ebdc6f174d29a1514a11569285c7680a9f73d689eff87aac6b
      cni.projectcalico.org/podIP: 10.1.249.50/32
      cni.projectcalico.org/podIPs: 10.1.249.50/32
    creationTimestamp: "2025-06-01T04:43:35Z"
    generateName: kube-prom-stack-kube-prome-operator-d45c777c4-
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      pod-template-hash: d45c777c4
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-operator-d45c777c4-rz282
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prom-stack-kube-prome-operator-d45c777c4
      uid: e8f6aaaf-a733-4a45-843a-9dcbece2d54d
    resourceVersion: "748843"
    uid: 428d0ab1-76ec-4274-967a-a4483209bb90
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/kube-prom-stack-kube-prome-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.38.0
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.82.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l2vvv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prom-stack-kube-prome-operator
    serviceAccountName: kube-prom-stack-kube-prome-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: kube-prom-stack-kube-prome-admission
    - name: kube-api-access-l2vvv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:42Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://660a27d38d9100d4ab2b05df402290d16d7b0354a958c8c372d16c433bd0b135
      image: quay.io/prometheus-operator/prometheus-operator:v0.82.2
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:1875f5418dbea75e79177fdadabcaa4aca69ea73a591aa463550f1d44b86393d
      lastState:
        terminated:
          containerID: containerd://bb3df323f5ded98238ce5eb8a2a95258bfd0c94afee505c86c56c02e67047e65
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:18Z"
      name: kube-prometheus-stack
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:41Z"
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l2vvv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.50
    podIPs:
    - ip: 10.1.249.50
    qosClass: BestEffort
    startTime: "2025-06-01T04:43:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: 5179bf586cb0f38a833a9678cb16debe4f35a6e7b447eaaac65854ad1044735f
      cni.projectcalico.org/podIP: 10.1.249.54/32
      cni.projectcalico.org/podIPs: 10.1.249.54/32
    creationTimestamp: "2025-06-01T04:43:35Z"
    generateName: kube-prom-stack-kube-state-metrics-766df6c6fd-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.33.2
      pod-template-hash: 766df6c6fd
      release: kube-prom-stack
    name: kube-prom-stack-kube-state-metrics-766df6c6fd-lmn9j
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-prom-stack-kube-state-metrics-766df6c6fd
      uid: 12461a37-1e59-4314-ba78-ce257bd63d42
    resourceVersion: "748876"
    uid: bf004535-a1e5-4c92-8cf9-fe13017b0b8c
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-465fb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prom-stack-kube-state-metrics
    serviceAccountName: kube-prom-stack-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-465fb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9d1d72f7a8c3d7a5f10e4d100e7eae56e6355c6073e63e5feed5590d628b884e
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:db384bf43222b066c378e77027a675d4cd9911107adba46c2922b3a55e10d6fb
      lastState:
        terminated:
          containerID: containerd://747f9736a6dc77ea3b4f2c6a3eedd9165cb8086eeb27432c63c8094f7f89f0f6
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:18Z"
      name: kube-state-metrics
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:32Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-465fb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.54
    podIPs:
    - ip: 10.1.249.54
    qosClass: BestEffort
    startTime: "2025-06-01T04:43:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-06-01T04:43:35Z"
    generateName: kube-prom-stack-prometheus-node-exporter-
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.1
      controller-revision-hash: d69486d4
      helm.sh/chart: prometheus-node-exporter-4.46.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: kube-prom-stack
    name: kube-prom-stack-prometheus-node-exporter-n4rkr
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-prom-stack-prometheus-node-exporter
      uid: 259ac45b-f530-41f2-a608-838e6e8acbf3
    resourceVersion: "748663"
    uid: 23f01bb5-e5c1-4e98-b371-650f0401ce1f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - leo-vostro
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.9.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: leo-vostro
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: kube-prom-stack-prometheus-node-exporter
    serviceAccountName: kube-prom-stack-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6177d4db263b41ca8b72890e8a67782b67e40a08514620018aad25566d4f10bc
      image: quay.io/prometheus/node-exporter:v1.9.1
      imageID: quay.io/prometheus/node-exporter@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a
      lastState:
        terminated:
          containerID: containerd://9b402f65f949b2a3ba709e5340fe08311bfd764abb2ab1c204f44dc048e53947
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:15Z"
      name: node-exporter
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:27Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 192.168.1.11
    podIPs:
    - ip: 192.168.1.11
    qosClass: BestEffort
    startTime: "2025-06-01T04:43:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 52f2160434f6afa7add4ee73677fad071033a893bbbc78055199fd85bef1af93
      cni.projectcalico.org/containerID: 8e141d633d0d4276b5d439855976b670ed2c388053317fe1d8ceaefc4e8281b5
      cni.projectcalico.org/podIP: 10.1.249.33/32
      cni.projectcalico.org/podIPs: 10.1.249.33/32
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-06-01T04:47:12Z"
    generateName: loki-
    generation: 1
    labels:
      app: loki
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-7465cf4474
      name: loki
      release: loki
      statefulset.kubernetes.io/pod-name: loki-0
    name: loki-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: d8478209-2a9d-45a0-9736-a898b784695f
    resourceVersion: "749043"
    uid: 2cd583bc-fa30-4472-a930-ec8108345712
  spec:
    affinity: {}
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.9.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: memberlist-port
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vtncw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-0
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: config
      secret:
        defaultMode: 420
        secretName: loki
    - emptyDir: {}
      name: storage
    - name: kube-api-access-vtncw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:47:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:52:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:52:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:47:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6af3dfed2356b13dc68617bd1b8799069cc0a4327228b66afd3695209f801ddf
      image: docker.io/grafana/loki:2.9.3
      imageID: docker.io/grafana/loki@sha256:eb92f1a439171542fd718f929fad38c917b3cad15ec830ba4742e2ba5ab03313
      lastState:
        terminated:
          containerID: containerd://1a156461509d3ef4cc3661b9260efce7669dcd12e79a4473afb0002cb21790e6
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:20Z"
      name: loki
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:31Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vtncw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.33
    podIPs:
    - ip: 10.1.249.33
    qosClass: BestEffort
    startTime: "2025-06-01T04:47:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 52f2160434f6afa7add4ee73677fad071033a893bbbc78055199fd85bef1af93
      cni.projectcalico.org/containerID: f5ce1cce3246685f6b3b52e9429bc4cad37e3647d4218b5d1b131234351bb813
      cni.projectcalico.org/podIP: 10.1.249.25/32
      cni.projectcalico.org/podIPs: 10.1.249.25/32
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-06-01T04:48:24Z"
    generateName: loki-
    generation: 1
    labels:
      app: loki
      apps.kubernetes.io/pod-index: "1"
      controller-revision-hash: loki-7465cf4474
      name: loki
      release: loki
      statefulset.kubernetes.io/pod-name: loki-1
    name: loki-1
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: d8478209-2a9d-45a0-9736-a898b784695f
    resourceVersion: "749054"
    uid: 756d5185-e1a7-4ff4-a37f-d140fa73511a
  spec:
    affinity: {}
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.9.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: memberlist-port
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nvmf9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-1
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: config
      secret:
        defaultMode: 420
        secretName: loki
    - emptyDir: {}
      name: storage
    - name: kube-api-access-nvmf9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:48:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:52:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:52:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:48:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3a662f986f45479b8087e374a1f1d54325752d9a58454f41fcbb49638ac6680d
      image: docker.io/grafana/loki:2.9.3
      imageID: docker.io/grafana/loki@sha256:eb92f1a439171542fd718f929fad38c917b3cad15ec830ba4742e2ba5ab03313
      lastState:
        terminated:
          containerID: containerd://0a7ba7cfdddca39acf69d669296c17862f84089ae1700d2f99510c045b56cd6e
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:20Z"
      name: loki
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:32Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nvmf9
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.25
    podIPs:
    - ip: 10.1.249.25
    qosClass: BestEffort
    startTime: "2025-06-01T04:48:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
      cni.projectcalico.org/containerID: 12b83c539e81a3975d6551a4f9b95bb9ba737fd6a4bfd4df00bafeee8b44eeb4
      cni.projectcalico.org/podIP: 10.1.249.11/32
      cni.projectcalico.org/podIPs: 10.1.249.11/32
    creationTimestamp: "2025-06-01T04:47:12Z"
    generateName: loki-promtail-
    generation: 1
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: fc468894f
      pod-template-generation: "1"
    name: loki-promtail-mmtnq
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: 38b9ce72-3637-42cb-912b-7afcfd83da82
    resourceVersion: "748893"
    uid: c088f8f1-45a6-4e45-8b1b-36c5dc355f5d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - leo-vostro
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.9.3
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ccnmn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-ccnmn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:47:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:47:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://46e03569d84d54a96f490cc3d3b3ff41078d0be7bc9836a6fd15fe649cd4c561
      image: docker.io/grafana/promtail:2.9.3
      imageID: docker.io/grafana/promtail@sha256:b338a29de45ef8ffa96f882f3a36306b1e61262b2a560ff523e0e2633cccbbc4
      lastState:
        terminated:
          containerID: containerd://5fc6724b7874c880a8dbebebd41a7fae4b2520f5cd298f573cfb2ddc81dfe75a
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:19Z"
      name: promtail
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:39Z"
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ccnmn
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.11
    podIPs:
    - ip: 10.1.249.11
    qosClass: BestEffort
    startTime: "2025-06-01T04:47:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/containerID: f2ea378a28efa16b2e25bf3d5e9b8dd40f1cff0e3809f8114a30f86f7ead64b9
      cni.projectcalico.org/podIP: 10.1.249.1/32
      cni.projectcalico.org/podIPs: 10.1.249.1/32
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2025-06-01T04:43:37Z"
    generateName: prometheus-kube-prom-stack-kube-prome-prometheus-
    generation: 1
    labels:
      app.kubernetes.io/instance: kube-prom-stack-kube-prome-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 3.4.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-kube-prom-stack-kube-prome-prometheus-6dcb9f95c
      operator.prometheus.io/name: kube-prom-stack-kube-prome-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: kube-prom-stack-kube-prome-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-kube-prom-stack-kube-prome-prometheus-0
    name: prometheus-kube-prom-stack-kube-prome-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-kube-prom-stack-kube-prome-prometheus
      uid: 2c4c93e6-ba26-4d1a-b0a0-ec721d496d84
    resourceVersion: "748900"
    uid: a5049ce7-d79c-449b-be16-7b07cf3cd964
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - kube-prom-stack-kube-prome-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://kube-prom-stack-kube-prome-prometheus.monitoring:9090
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=10d
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v3.4.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 5
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: stack-pvc-prometheus
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5t2k
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5t2k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-kube-prom-stack-kube-prome-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5t2k
        readOnly: true
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: kube-prom-stack-kube-prome-prometheus
    serviceAccountName: kube-prom-stack-kube-prome-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: stack-pvc-prometheus
      persistentVolumeClaim:
        claimName: stack-pvc-prometheus-prometheus-kube-prom-stack-kube-prome-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prom-stack-kube-prome-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-kube-prom-stack-kube-prome-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-kube-prom-stack-kube-prome-prometheus-web-config
    - name: kube-api-access-l5t2k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:43:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1697fd6f4f6e20018b43a39b2bab1c0a25aee2623ccf523a0fb48256bcff9126
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:710458fdccd42d56bb5b20b548fe9a2f832c12aa26256e80cda48a9edb0454ff
      lastState:
        terminated:
          containerID: containerd://ec85bdb299845f343122a4a4b1211d016c67b7c6b375d9d5a9a8859bbccf557a
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          message: |
            ts=2025-06-01T19:46:25.859202024Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:148 msg="Starting prometheus-config-reloader" version="(version=0.82.2, branch=, revision=4d008d5)" build_context="(go=go1.24.3, platform=linux/amd64, user=, date=20250512-08:50:19, tags=unknown)"
            ts=2025-06-01T19:46:25.859581436Z level=info caller=/workspace/internal/goruntime/cpu.go:27 msg="Leaving GOMAXPROCS=8: CPU quota undefined"
            level=info ts=2025-06-01T19:46:25.860438132Z caller=reloader.go:282 msg="reloading via HTTP"
            ts=2025-06-01T19:46:25.860434961Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:202 msg="Starting web server for metrics" listen=:8080
            ts=2025-06-01T19:46:25.860788098Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:347 msg="Listening on" address=[::]:8080
            ts=2025-06-01T19:46:25.860821653Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:350 msg="TLS is disabled." http2=false address=[::]:8080
            level=error ts=2025-06-01T19:46:25.866770449Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=info ts=2025-06-01T19:46:30.931813477Z caller=reloader.go:548 msg="Reload triggered" cfg_in=/etc/prometheus/config/prometheus.yaml.gz cfg_out=/etc/prometheus/config_out/prometheus.env.yaml cfg_dirs= watched_dirs=/etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
            level=info ts=2025-06-01T19:46:30.931887029Z caller=reloader.go:330 msg="started watching config file and directories for changes" cfg=/etc/prometheus/config/prometheus.yaml.gz cfgDirs= out=/etc/prometheus/config_out/prometheus.env.yaml dirs=/etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
          reason: Unknown
          startedAt: "2025-06-01T19:46:25Z"
      name: config-reloader
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:43Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5t2k
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://9562aaaff2672ea9474a0be0fa1385a2ae78f33030532ee2e7fc500bc6fee428
      image: quay.io/prometheus/prometheus:v3.4.1
      imageID: quay.io/prometheus/prometheus@sha256:9abc6cf6aea7710d163dbb28d8eeb7dc5baef01e38fa4cd146a406dd9f07f70d
      lastState:
        terminated:
          containerID: containerd://899789550c8c238a5b2c915ca06b84b1b81399c5d7335ab38199e8e7f5633434
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          message: |
            component="scrape manager" scrape_pool=serviceMonitor/monitoring/kube-prom-stack-kube-prome-kubelet/1 target=https://192.168.1.11:10250/metrics/cadvisor num_dropped=391
            time=2025-06-03T00:33:28.116Z level=WARN source=scrape.go:1905 msg="Error on ingesting samples that are too old or are too far into the future" component="scrape manager" scrape_pool=serviceMonitor/monitoring/kube-prom-stack-kube-prome-kubelet/1 target=https://192.168.1.11:10250/metrics/cadvisor num_dropped=196
            time=2025-06-03T00:35:58.668Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:36:12.677Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:36:31.675Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:39:07.679Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:41:16.680Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:44:42.677Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:44:46.671Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:46:26.682Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
            time=2025-06-03T00:48:15.681Z level=INFO source=warnings.go:70 msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice" component=k8s_client_runtime
          reason: Unknown
          startedAt: "2025-06-01T19:46:25Z"
      name: prometheus
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:43Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /prometheus
        name: stack-pvc-prometheus
      - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5t2k
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    initContainerStatuses:
    - containerID: containerd://f1b9bf53dddcfe350b8de96e2cde23ebefe6064d39ef529c020e4feb81d6bace
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:710458fdccd42d56bb5b20b548fe9a2f832c12aa26256e80cda48a9edb0454ff
      lastState: {}
      name: init-config-reloader
      ready: true
      resources: {}
      restartCount: 2
      started: false
      state:
        terminated:
          containerID: containerd://f1b9bf53dddcfe350b8de96e2cde23ebefe6064d39ef529c020e4feb81d6bace
          exitCode: 0
          finishedAt: "2025-06-03T00:51:42Z"
          reason: Completed
          startedAt: "2025-06-03T00:51:42Z"
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5t2k
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.1.249.1
    podIPs:
    - ip: 10.1.249.1
    qosClass: BestEffort
    startTime: "2025-06-01T04:43:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 6bd3b3f1438e05710dd9abcb76f71c2199bf38133349a5ce61e88a4b4b9471c8
      cni.projectcalico.org/containerID: 54852fde075cb2899e8f43bc8e76696a58168d374dea5017888ef71099da1eb1
      cni.projectcalico.org/podIP: 10.1.249.62/32
      cni.projectcalico.org/podIPs: 10.1.249.62/32
    creationTimestamp: "2025-06-01T04:49:24Z"
    generateName: tempo-
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: tempo-584c8c8b9c
      statefulset.kubernetes.io/pod-name: tempo-0
    name: tempo-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: tempo
      uid: e5637819-1489-4f6b-a7e5-27c1139df61f
    resourceVersion: "748991"
    uid: c0636919-4f8f-43aa-97b4-96969cb192e8
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/conf/tempo.yaml
      - -mem-ballast-size-mbs=1024
      image: grafana/tempo:2.7.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3100
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: tempo
      ports:
      - containerPort: 3100
        name: prom-metrics
        protocol: TCP
      - containerPort: 6831
        name: jaeger-thrift-c
        protocol: UDP
      - containerPort: 6832
        name: jaeger-thrift-b
        protocol: UDP
      - containerPort: 14268
        name: jaeger-thrift-h
        protocol: TCP
      - containerPort: 14250
        name: jaeger-grpc
        protocol: TCP
      - containerPort: 9411
        name: zipkin
        protocol: TCP
      - containerPort: 55680
        name: otlp-legacy
        protocol: TCP
      - containerPort: 4317
        name: otlp-grpc
        protocol: TCP
      - containerPort: 55681
        name: otlp-httplegacy
        protocol: TCP
      - containerPort: 4318
        name: otlp-http
        protocol: TCP
      - containerPort: 55678
        name: opencensus
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 3100
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/tempo
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tzfhx
        readOnly: true
    - args:
      - -config
      - /conf/tempo-query.yaml
      image: grafana/tempo-query:2.7.1
      imagePullPolicy: IfNotPresent
      name: tempo-query
      ports:
      - containerPort: 16686
        name: jaeger-ui
        protocol: TCP
      - containerPort: 16687
        name: jaeger-metrics
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: tempo-query-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tzfhx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: tempo-0
    nodeName: leo-vostro
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: tempo
    serviceAccountName: tempo
    subdomain: tempo-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-tempo-0
    - configMap:
        defaultMode: 420
        name: tempo-query
      name: tempo-query-conf
    - configMap:
        defaultMode: 420
        name: tempo
      name: tempo-conf
    - name: kube-api-access-tzfhx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:51:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:49:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:52:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-03T00:52:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-01T04:49:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: 500m
        memory: 512M
      containerID: containerd://987300b139f7ac0f3fb261cbd31791766b4ab299bdfd93c151b29c586fbfb9ca
      image: docker.io/grafana/tempo:2.7.1
      imageID: docker.io/grafana/tempo@sha256:4443be217c396b065ee34845534199c36fdba4dc619cb96550e228d73fba6e69
      lastState:
        terminated:
          containerID: containerd://d296a34af2eb402a5b5e3ddef2db77d770376eac21ed6be560802f534d3fc1f9
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:18Z"
      name: tempo
      ready: true
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 512M
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:43Z"
      volumeMounts:
      - mountPath: /conf
        name: tempo-conf
      - mountPath: /var/tempo
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tzfhx
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://1115293e38b890e05a219f72ae8b5a061029712caafd719a8cdddd896917d2f4
      image: docker.io/grafana/tempo-query:2.7.1
      imageID: docker.io/grafana/tempo-query@sha256:e92491e0319d84feaa716d21650ea4410712236017dd7f1375559974100bb14d
      lastState:
        terminated:
          containerID: containerd://8118295def3ee5594f18801dbb96f6b80f9cd1450bb59a95f85c9719079221d3
          exitCode: 255
          finishedAt: "2025-06-03T00:51:02Z"
          reason: Unknown
          startedAt: "2025-06-01T19:46:19Z"
      name: tempo-query
      ready: true
      resources: {}
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-06-03T00:51:43Z"
      volumeMounts:
      - mountPath: /conf
        name: tempo-query-conf
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tzfhx
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.1.11
    hostIPs:
    - ip: 192.168.1.11
    phase: Running
    podIP: 10.1.249.62
    podIPs:
    - ip: 10.1.249.62
    qosClass: Burstable
    startTime: "2025-06-01T04:49:28Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-05-08T00:46:37Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "73"
    uid: 85473839-a6b5-43f6-bee1-7db6d4e4cd5a
  spec:
    clusterIP: 10.152.183.1
    clusterIPs:
    - 10.152.183.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 16443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/port":"9153","prometheus.io/scrape":"true"},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/name":"CoreDNS"},"name":"kube-dns","namespace":"kube-system"},"spec":{"clusterIP":"10.152.183.10","ports":[{"name":"dns","port":53,"protocol":"UDP"},{"name":"dns-tcp","port":53,"protocol":"TCP"},{"name":"metrics","port":9153,"protocol":"TCP"}],"selector":{"k8s-app":"kube-dns"}}}
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-05-08T00:46:39Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "170"
    uid: 996cf953-15ed-47d3-8069-9d06116d0ced
  spec:
    clusterIP: 10.152.183.10
    clusterIPs:
    - 10.152.183.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      jobLabel: coredns
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-coredns
    namespace: kube-system
    resourceVersion: "726894"
    uid: 69c181c8-0dda-4bdc-bd5c-28214e5bf5bc
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      jobLabel: kube-controller-manager
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-kube-controller-manager
    namespace: kube-system
    resourceVersion: "726895"
    uid: 2afe7b54-ce3d-4b95-807d-4303f2189f6c
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    selector:
      component: kube-controller-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      jobLabel: kube-etcd
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-kube-etcd
    namespace: kube-system
    resourceVersion: "726893"
    uid: 823444de-cef5-4733-99c5-973e0d4dff9f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-kube-proxy
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      jobLabel: kube-proxy
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-kube-proxy
    namespace: kube-system
    resourceVersion: "726896"
    uid: 1b4b68d8-5f88-4a25-a80f-0d15726b160e
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: kube-proxy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      jobLabel: kube-scheduler
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-kube-scheduler
    namespace: kube-system
    resourceVersion: "726892"
    uid: 680ffa99-6ad4-4242-be06-623a0aefec6f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    selector:
      component: kube-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-06-01T01:46:40Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: kube-prom-stack-kube-prome-kubelet
    namespace: kube-system
    resourceVersion: "699292"
    uid: 732c9a73-2376-41e1-aa2c-1352a8658543
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kubelet"},"name":"kubelet","namespace":"kube-system"},"spec":{"ports":[{"name":"https-metrics","port":10250,"targetPort":10250}],"selector":{"k8s-app":"kubelet"},"type":"ClusterIP"}}
    creationTimestamp: "2025-06-03T01:38:53Z"
    labels:
      k8s-app: kubelet
    name: kubelet
    namespace: kube-system
    resourceVersion: "755720"
    uid: 72e4dc8a-639d-4e47-88a0-3c5a96c9961a
  spec:
    clusterIP: 10.152.183.228
    clusterIPs:
    - 10.152.183.228
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    selector:
      k8s-app: kubelet
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"}}}
    creationTimestamp: "2025-06-03T01:27:27Z"
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "754100"
    uid: 86398893-00c7-4c52-abe0-4be5c3ba446c
  spec:
    clusterIP: 10.152.183.147
    clusterIPs:
    - 10.152.183.147
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"metallb-webhook-service","namespace":"metallb-system"},"spec":{"ports":[{"port":443,"targetPort":9443}],"selector":{"component":"controller"}}}
    creationTimestamp: "2025-05-08T00:51:23Z"
    name: metallb-webhook-service
    namespace: metallb-system
    resourceVersion: "1074"
    uid: 9c0d3c79-2c7f-4733-b9ed-49e63eccf400
  spec:
    clusterIP: 10.152.183.187
    clusterIPs:
    - 10.152.183.187
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 443
      protocol: TCP
      targetPort: 9443
    selector:
      component: controller
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-06-01T04:43:36Z"
    labels:
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: kube-prom-stack-kube-prome-alertmanager
      uid: aa5f678e-225d-4355-8bad-cf5e17da7d8a
    resourceVersion: "727089"
    uid: 15577ef9-7519-4039-9be1-eb334527f02a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: black
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-03T01:15:34Z"
    labels:
      app.kubernetes.io/instance: black
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-blackbox-exporter
      app.kubernetes.io/version: v0.26.0
      helm.sh/chart: prometheus-blackbox-exporter-9.8.0
    name: black-prometheus-blackbox-exporter
    namespace: monitoring
    resourceVersion: "752363"
    uid: 45a83884-aed6-4619-9025-cf9ec54d5491
  spec:
    clusterIP: 10.152.183.48
    clusterIPs:
    - 10.152.183.48
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9115
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: black
      app.kubernetes.io/name: prometheus-blackbox-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.0-security-01
      helm.sh/chart: grafana-9.2.1
    name: kube-prom-stack-grafana
    namespace: monitoring
    resourceVersion: "726932"
    uid: 802c7905-a65f-4399-90d5-1ed8268db435
  spec:
    clusterIP: 10.152.183.78
    clusterIPs:
    - 10.152.183.78
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      metallb.io/ip-allocated-from-pool: default-addresspool
    creationTimestamp: "2025-06-01T02:30:14Z"
    labels:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.0-security-01
      helm.sh/chart: grafana-9.2.1
    name: kube-prom-stack-grafana-ext
    namespace: monitoring
    resourceVersion: "706525"
    uid: 9872d605-fa17-4fb9-bb83-7436063e7e2d
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.152.183.115
    clusterIPs:
    - 10.152.183.115
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 32095
      port: 3000
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.1.200
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      release: kube-prom-stack
      self-monitor: "true"
    name: kube-prom-stack-kube-prome-alertmanager
    namespace: monitoring
    resourceVersion: "726935"
    uid: 4045b3ab-397d-4e56-80db-f366e6b91813
  spec:
    clusterIP: 10.152.183.35
    clusterIPs:
    - 10.152.183.35
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: kube-prom-stack-kube-prome-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-operator
    namespace: monitoring
    resourceVersion: "726911"
    uid: 075775a2-cdeb-4305-980b-bbdc91146e6f
  spec:
    clusterIP: 10.152.183.116
    clusterIPs:
    - 10.152.183.116
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: kube-prom-stack
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      release: kube-prom-stack
      self-monitor: "true"
    name: kube-prom-stack-kube-prome-prometheus
    namespace: monitoring
    resourceVersion: "726920"
    uid: ef73a3cc-c0f2-42f4-9c8d-81e5d61859d3
  spec:
    clusterIP: 10.152.183.246
    clusterIPs:
    - 10.152.183.246
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: kube-prom-stack-kube-prome-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.33.2
      release: kube-prom-stack
    name: kube-prom-stack-kube-state-metrics
    namespace: monitoring
    resourceVersion: "726923"
    uid: e2fd3e63-c766-4643-9139-4ba8f39169f4
  spec:
    clusterIP: 10.152.183.59
    clusterIPs:
    - 10.152.183.59
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-06-01T04:43:35Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.1
      helm.sh/chart: prometheus-node-exporter-4.46.1
      jobLabel: node-exporter
      release: kube-prom-stack
    name: kube-prom-stack-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "726928"
    uid: 5d243ede-0650-4302-99a2-069eabd8f9f1
  spec:
    clusterIP: 10.152.183.193
    clusterIPs:
    - 10.152.183.193
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:47:11Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki
    namespace: monitoring
    resourceVersion: "727816"
    uid: 0864bf0b-3a25-4937-b724-77bea36d4f4c
  spec:
    clusterIP: 10.152.183.215
    clusterIPs:
    - 10.152.183.215
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:47:11Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
      variant: headless
    name: loki-headless
    namespace: monitoring
    resourceVersion: "727809"
    uid: 69ef6c5e-7f92-4cf2-910a-f56bfaa88c15
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:47:11Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki-memberlist
    namespace: monitoring
    resourceVersion: "727808"
    uid: c270b4c0-a50e-4c60-a3d9-a0ad115b3a81
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7946
      protocol: TCP
      targetPort: memberlist-port
    publishNotReadyAddresses: true
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-06-01T04:43:37Z"
    labels:
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: kube-prom-stack-kube-prome-prometheus
      uid: eddcdbe2-0008-4a82-870c-70c830add687
    resourceVersion: "727110"
    uid: 4f47cfd9-c2f7-4efc-82ee-664de8c78ecc
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      metallb.io/ip-allocated-from-pool: default-addresspool
    creationTimestamp: "2025-06-01T02:40:25Z"
    labels:
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated-ext
    namespace: monitoring
    resourceVersion: "707977"
    uid: eca03a12-69f4-4557-b714-79550725ee49
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.152.183.135
    clusterIPs:
    - 10.152.183.135
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 31111
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.1.201
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:49:24Z"
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.7.1
      helm.sh/chart: tempo-1.21.1
    name: tempo
    namespace: monitoring
    resourceVersion: "728283"
    uid: 5d17d9c4-a22c-478d-8973-6bea991aad93
  spec:
    clusterIP: 10.152.183.224
    clusterIPs:
    - 10.152.183.224
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tempo-jaeger-thrift-compact
      port: 6831
      protocol: UDP
      targetPort: 6831
    - name: tempo-jaeger-thrift-binary
      port: 6832
      protocol: UDP
      targetPort: 6832
    - name: tempo-prom-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: jaeger-metrics
      port: 16687
      protocol: TCP
      targetPort: 16687
    - name: tempo-query-jaeger-ui
      port: 16686
      protocol: TCP
      targetPort: 16686
    - name: tempo-jaeger-thrift-http
      port: 14268
      protocol: TCP
      targetPort: 14268
    - name: grpc-tempo-jaeger
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: tempo-zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    - name: tempo-otlp-legacy
      port: 55680
      protocol: TCP
      targetPort: 55680
    - name: tempo-otlp-http-legacy
      port: 55681
      protocol: TCP
      targetPort: 55681
    - name: grpc-tempo-otlp
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: tempo-otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: tempo-opencensus
      port: 55678
      protocol: TCP
      targetPort: 55678
    selector:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/name: tempo
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"WAIT_FOR_DATASTORE","value":"true"},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"CALICO_NETWORKING_BACKEND","valueFrom":{"configMapKeyRef":{"key":"calico_backend","name":"calico-config"}}},{"name":"CLUSTER_TYPE","value":"k8s,bgp"},{"name":"IP","value":"autodetect"},{"name":"IP_AUTODETECTION_METHOD","value":"first-found"},{"name":"CALICO_IPV4POOL_VXLAN","value":"Always"},{"name":"CALICO_IPV6POOL_VXLAN","value":"Never"},{"name":"FELIX_IPINIPMTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"FELIX_VXLANMTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"FELIX_WIREGUARDMTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"CALICO_IPV4POOL_CIDR","value":"10.1.0.0/16"},{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_HEALTHENABLED","value":"true"},{"name":"FELIX_FEATUREDETECTOVERRIDE","value":"ChecksumOffloadBroken=true"}],"envFrom":[{"configMapRef":{"name":"kubernetes-services-endpoint","optional":true}}],"image":"docker.io/calico/node:v3.29.3","imagePullPolicy":"IfNotPresent","lifecycle":{"preStop":{"exec":{"command":["/bin/calico-node","-shutdown"]}}},"livenessProbe":{"exec":{"command":["/bin/calico-node","-felix-live"]},"failureThreshold":6,"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":10},"name":"calico-node","readinessProbe":{"exec":{"command":["/bin/calico-node","-felix-ready"]},"periodSeconds":10,"timeoutSeconds":10},"resources":{"requests":{"cpu":"250m"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir","readOnly":false},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/run/xtables.lock","name":"xtables-lock","readOnly":false},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false},{"mountPath":"/var/lib/calico","name":"var-lib-calico","readOnly":false},{"mountPath":"/var/run/nodeagent","name":"policysync"},{"mountPath":"/var/log/calico/cni","name":"cni-log-dir","readOnly":true}]}],"hostNetwork":true,"initContainers":[{"command":["/opt/cni/bin/calico-ipam","-upgrade"],"env":[{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"CALICO_NETWORKING_BACKEND","valueFrom":{"configMapKeyRef":{"key":"calico_backend","name":"calico-config"}}}],"envFrom":[{"configMapRef":{"name":"kubernetes-services-endpoint","optional":true}}],"image":"docker.io/calico/cni:v3.29.3","imagePullPolicy":"IfNotPresent","name":"upgrade-ipam","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/cni/networks","name":"host-local-net-dir"},{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"}]},{"command":["/opt/cni/bin/install"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","valueFrom":{"configMapKeyRef":{"key":"cni_network_config","name":"calico-config"}}},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"CNI_MTU","valueFrom":{"configMapKeyRef":{"key":"veth_mtu","name":"calico-config"}}},{"name":"SLEEP","value":"false"},{"name":"CNI_NET_DIR","value":"/var/snap/microk8s/current/args/cni-network"}],"envFrom":[{"configMapRef":{"name":"kubernetes-services-endpoint","optional":true}}],"image":"docker.io/calico/cni:v3.29.3","imagePullPolicy":"IfNotPresent","name":"install-cni","securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-node-critical","securityContext":{"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"calico-node","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/var/snap/microk8s/current/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/var/snap/microk8s/current/var/lib/calico"},"name":"var-lib-calico"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"},{"hostPath":{"path":"/sys/fs/","type":"DirectoryOrCreate"},"name":"sys-fs"},{"hostPath":{"path":"/var/snap/microk8s/current/opt/cni/bin","type":"DirectoryOrCreate"},"name":"cni-bin-dir"},{"hostPath":{"path":"/var/snap/microk8s/current/args/cni-network"},"name":"cni-net-dir"},{"hostPath":{"path":"/var/snap/microk8s/common/var/log/calico/cni"},"name":"cni-log-dir"},{"hostPath":{"path":"/var/snap/microk8s/current/var/lib/cni/networks"},"name":"host-local-net-dir"},{"hostPath":{"path":"/var/snap/microk8s/current/var/run/nodeagent","type":"DirectoryOrCreate"},"name":"policysync"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
    creationTimestamp: "2025-05-08T00:46:39Z"
    generation: 1
    labels:
      k8s-app: calico-node
    name: calico-node
    namespace: kube-system
    resourceVersion: "632145"
    uid: 68175b7f-5af5-479f-b7e5-fe15e1504fd2
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: WAIT_FOR_DATASTORE
            value: "true"
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CALICO_NETWORKING_BACKEND
            valueFrom:
              configMapKeyRef:
                key: calico_backend
                name: calico-config
          - name: CLUSTER_TYPE
            value: k8s,bgp
          - name: IP
            value: autodetect
          - name: IP_AUTODETECTION_METHOD
            value: first-found
          - name: CALICO_IPV4POOL_VXLAN
            value: Always
          - name: CALICO_IPV6POOL_VXLAN
            value: Never
          - name: FELIX_IPINIPMTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: FELIX_VXLANMTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: FELIX_WIREGUARDMTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: CALICO_IPV4POOL_CIDR
            value: 10.1.0.0/16
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "true"
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: FELIX_HEALTHENABLED
            value: "true"
          - name: FELIX_FEATUREDETECTOVERRIDE
            value: ChecksumOffloadBroken=true
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: docker.io/calico/node:v3.29.3
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/calico-node
                - -shutdown
          livenessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-live
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-node
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -felix-ready
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            requests:
              cpu: 250m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /var/run/calico
            name: var-run-calico
          - mountPath: /var/lib/calico
            name: var-lib-calico
          - mountPath: /var/run/nodeagent
            name: policysync
          - mountPath: /var/log/calico/cni
            name: cni-log-dir
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /opt/cni/bin/calico-ipam
          - -upgrade
          env:
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CALICO_NETWORKING_BACKEND
            valueFrom:
              configMapKeyRef:
                key: calico_backend
                name: calico-config
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: docker.io/calico/cni:v3.29.3
          imagePullPolicy: IfNotPresent
          name: upgrade-ipam
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/cni/networks
            name: host-local-net-dir
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
        - command:
          - /opt/cni/bin/install
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: CNI_NETWORK_CONFIG
            valueFrom:
              configMapKeyRef:
                key: cni_network_config
                name: calico-config
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CNI_MTU
            valueFrom:
              configMapKeyRef:
                key: veth_mtu
                name: calico-config
          - name: SLEEP
            value: "false"
          - name: CNI_NET_DIR
            value: /var/snap/microk8s/current/args/cni-network
          envFrom:
          - configMapRef:
              name: kubernetes-services-endpoint
              optional: true
          image: docker.io/calico/cni:v3.29.3
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: calico-node
        serviceAccountName: calico-node
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /var/snap/microk8s/current/var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /var/snap/microk8s/current/var/lib/calico
            type: ""
          name: var-lib-calico
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /sys/fs/
            type: DirectoryOrCreate
          name: sys-fs
        - hostPath:
            path: /var/snap/microk8s/current/opt/cni/bin
            type: DirectoryOrCreate
          name: cni-bin-dir
        - hostPath:
            path: /var/snap/microk8s/current/args/cni-network
            type: ""
          name: cni-net-dir
        - hostPath:
            path: /var/snap/microk8s/common/var/log/calico/cni
            type: ""
          name: cni-log-dir
        - hostPath:
            path: /var/snap/microk8s/current/var/lib/cni/networks
            type: ""
          name: host-local-net-dir
        - hostPath:
            path: /var/snap/microk8s/current/var/run/nodeagent
            type: DirectoryOrCreate
          name: policysync
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"app":"metallb","component":"speaker"},"name":"speaker","namespace":"metallb-system"},"spec":{"selector":{"matchLabels":{"app":"metallb","component":"speaker"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"7472","prometheus.io/scrape":"true"},"labels":{"app":"metallb","component":"speaker"}},"spec":{"containers":[{"args":["--port=7472","--log-level=info"],"env":[{"name":"METALLB_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"METALLB_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"METALLB_HOST","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}},{"name":"METALLB_ML_BIND_ADDR","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"METALLB_ML_LABELS","value":"app=metallb,component=speaker"},{"name":"METALLB_ML_SECRET_KEY_PATH","value":"/etc/ml_secret_key"}],"image":"quay.io/metallb/speaker:v0.14.9","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/metrics","port":"monitoring"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"name":"speaker","ports":[{"containerPort":7472,"name":"monitoring"},{"containerPort":7946,"name":"memberlist-tcp"},{"containerPort":7946,"name":"memberlist-udp","protocol":"UDP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/metrics","port":"monitoring"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"add":["NET_RAW"],"drop":["ALL"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/etc/ml_secret_key","name":"memberlist","readOnly":true},{"mountPath":"/etc/metallb","name":"metallb-excludel2","readOnly":true}]}],"hostNetwork":true,"nodeSelector":{"kubernetes.io/os":"linux"},"serviceAccountName":"speaker","terminationGracePeriodSeconds":2,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane","operator":"Exists"}],"volumes":[{"name":"memberlist","secret":{"defaultMode":420,"secretName":"memberlist"}},{"configMap":{"defaultMode":256,"name":"metallb-excludel2"},"name":"metallb-excludel2"}]}}}}
    creationTimestamp: "2025-05-08T00:51:23Z"
    generation: 1
    labels:
      app: metallb
      component: speaker
    name: speaker
    namespace: metallb-system
    resourceVersion: "632144"
    uid: 6843b657-7950-4adf-9c92-0a5760a77816
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: metallb
        component: speaker
    template:
      metadata:
        annotations:
          prometheus.io/port: "7472"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: metallb
          component: speaker
      spec:
        containers:
        - args:
          - --port=7472
          - --log-level=info
          env:
          - name: METALLB_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: METALLB_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: METALLB_HOST
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: METALLB_ML_BIND_ADDR
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: METALLB_ML_LABELS
            value: app=metallb,component=speaker
          - name: METALLB_ML_SECRET_KEY_PATH
            value: /etc/ml_secret_key
          image: quay.io/metallb/speaker:v0.14.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: speaker
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          - containerPort: 7946
            name: memberlist-tcp
            protocol: TCP
          - containerPort: 7946
            name: memberlist-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_RAW
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ml_secret_key
            name: memberlist
            readOnly: true
          - mountPath: /etc/metallb
            name: metallb-excludel2
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: speaker
        serviceAccountName: speaker
        terminationGracePeriodSeconds: 2
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: memberlist
          secret:
            defaultMode: 420
            secretName: memberlist
        - configMap:
            defaultMode: 256
            name: metallb-excludel2
          name: metallb-excludel2
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.1
      helm.sh/chart: prometheus-node-exporter-4.46.1
      release: kube-prom-stack
    name: kube-prom-stack-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "727064"
    uid: 259ac45b-f530-41f2-a608-838e6e8acbf3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prom-stack
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.9.1
          helm.sh/chart: prometheus-node-exporter-4.46.1
          jobLabel: node-exporter
          release: kube-prom-stack
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.9.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: kube-prom-stack-prometheus-node-exporter
        serviceAccountName: kube-prom-stack-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:47:11Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 2.9.3
      helm.sh/chart: promtail-6.15.5
    name: loki-promtail
    namespace: monitoring
    resourceVersion: "727889"
    uid: 38b9ce72-3637-42cb-912b-7afcfd83da82
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: promtail
      spec:
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:2.9.3
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: loki-promtail
        serviceAccountName: loki-promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: loki-promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-kube-controllers"},"name":"calico-kube-controllers","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"calico-kube-controllers"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"k8s-app":"calico-kube-controllers"},"name":"calico-kube-controllers","namespace":"kube-system"},"spec":{"containers":[{"env":[{"name":"ENABLED_CONTROLLERS","value":"node"},{"name":"DATASTORE_TYPE","value":"kubernetes"}],"image":"docker.io/calico/kube-controllers:v3.29.3","imagePullPolicy":"IfNotPresent","livenessProbe":{"exec":{"command":["/usr/bin/check-status","-l"]},"failureThreshold":6,"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":10},"name":"calico-kube-controllers","readinessProbe":{"exec":{"command":["/usr/bin/check-status","-r"]},"periodSeconds":10},"securityContext":{"runAsNonRoot":true}}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","securityContext":{"seccompProfile":{"type":"RuntimeDefault"}},"serviceAccountName":"calico-kube-controllers","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"}]}}}}
    creationTimestamp: "2025-05-08T00:46:39Z"
    generation: 1
    labels:
      k8s-app: calico-kube-controllers
    name: calico-kube-controllers
    namespace: kube-system
    resourceVersion: "632149"
    uid: 6dee672f-b90e-42b5-8233-0168642d7b4b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: calico-kube-controllers
        name: calico-kube-controllers
        namespace: kube-system
      spec:
        containers:
        - env:
          - name: ENABLED_CONTROLLERS
            value: node
          - name: DATASTORE_TYPE
            value: kubernetes
          image: docker.io/calico/kube-controllers:v3.29.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -l
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-kube-controllers
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: calico-kube-controllers
        serviceAccountName: calico-kube-controllers
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-05-08T00:46:45Z"
      lastUpdateTime: "2025-05-08T00:47:26Z"
      message: ReplicaSet "calico-kube-controllers-79949b87d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-05-27T23:30:31Z"
      lastUpdateTime: "2025-05-27T23:30:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/name":"CoreDNS"},"name":"coredns","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"kube-dns"}},"strategy":{"rollingUpdate":{"maxSurge":"10%","maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"priorityClassName":"system-cluster-critical"},"labels":{"k8s-app":"kube-dns"}},"spec":{"containers":[{"args":["-conf","/etc/coredns/Corefile"],"image":"coredns/coredns:1.12.1","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/health","port":8080,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"coredns","ports":[{"containerPort":53,"name":"dns","protocol":"UDP"},{"containerPort":53,"name":"dns-tcp","protocol":"TCP"},{"containerPort":9153,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/ready","port":8181,"scheme":"HTTP"}},"resources":{"limits":{"memory":"170Mi"},"requests":{"cpu":"100m","memory":"70Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/etc/coredns","name":"config-volume","readOnly":true}]}],"dnsPolicy":"Default","priorityClassName":"system-cluster-critical","serviceAccountName":"coredns","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"items":[{"key":"Corefile","path":"Corefile"}],"name":"coredns"},"name":"config-volume"}]}}}}
    creationTimestamp: "2025-05-29T01:11:33Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: coredns
    namespace: kube-system
    resourceVersion: "648727"
    uid: 48415390-8267-4330-8d0f-67b3877af255
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          priorityClassName: system-cluster-critical
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: coredns/coredns:1.12.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-05-29T01:11:35Z"
      lastUpdateTime: "2025-05-29T01:11:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-05-29T01:11:33Z"
      lastUpdateTime: "2025-05-29T01:11:35Z"
      message: ReplicaSet "coredns-ccd8f67bc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"hostpath-provisioner"},"name":"hostpath-provisioner","namespace":"kube-system"},"spec":{"replicas":1,"revisionHistoryLimit":0,"selector":{"matchLabels":{"k8s-app":"hostpath-provisioner"}},"template":{"metadata":{"labels":{"k8s-app":"hostpath-provisioner"}},"spec":{"containers":[{"env":[{"name":"NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"PV_DIR","value":"/var/snap/microk8s/common/default-storage"},{"name":"BUSYBOX_IMAGE","value":"busybox:1.28.4"}],"image":"cdkbot/hostpath-provisioner:1.5.0","name":"hostpath-provisioner","volumeMounts":[{"mountPath":"/var/snap/microk8s/common/default-storage","name":"pv-volume"}]}],"serviceAccountName":"microk8s-hostpath","volumes":[{"hostPath":{"path":"/var/snap/microk8s/common/default-storage"},"name":"pv-volume"}]}}}}
    creationTimestamp: "2025-05-29T01:11:41Z"
    generation: 1
    labels:
      k8s-app: hostpath-provisioner
    name: hostpath-provisioner
    namespace: kube-system
    resourceVersion: "742499"
    uid: 09839664-f1e0-47f6-920b-dfa292304339
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: hostpath-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hostpath-provisioner
      spec:
        containers:
        - env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: PV_DIR
            value: /var/snap/microk8s/common/default-storage
          - name: BUSYBOX_IMAGE
            value: busybox:1.28.4
          image: cdkbot/hostpath-provisioner:1.5.0
          imagePullPolicy: IfNotPresent
          name: hostpath-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/snap/microk8s/common/default-storage
            name: pv-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: microk8s-hostpath
        serviceAccountName: microk8s-hostpath
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/snap/microk8s/common/default-storage
            type: ""
          name: pv-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-05-29T01:11:41Z"
      lastUpdateTime: "2025-05-29T01:11:43Z"
      message: ReplicaSet "hostpath-provisioner-c778b7559" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-06-01T20:26:55Z"
      lastUpdateTime: "2025-06-01T20:26:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metrics-server"}},"strategy":{"rollingUpdate":{"maxUnavailable":0}},"template":{"metadata":{"labels":{"k8s-app":"metrics-server"}},"spec":{"containers":[{"args":["--cert-dir=/tmp","--secure-port=4443","--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname","--kubelet-use-node-status-port","--metric-resolution=15s","--kubelet-insecure-tls"],"image":"registry.k8s.io/metrics-server/metrics-server:v0.7.2","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":"https","scheme":"HTTPS"},"periodSeconds":10},"name":"metrics-server","ports":[{"containerPort":4443,"name":"https","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/readyz","port":"https","scheme":"HTTPS"},"initialDelaySeconds":20,"periodSeconds":10},"resources":{"requests":{"cpu":"100m","memory":"200Mi"}},"securityContext":{"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":1000},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-dir"}]}],"nodeSelector":{"kubernetes.io/arch":"amd64","kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","volumes":[{"emptyDir":{},"name":"tmp-dir"}]}}}}
    creationTimestamp: "2025-06-03T01:27:27Z"
    generation: 1
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "754184"
    uid: 74f3c997-a37d-46ac-a670-5a2e9b3133d1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=4443
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --kubelet-insecure-tls
          image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 4443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/arch: amd64
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-06-03T01:27:49Z"
      lastUpdateTime: "2025-06-03T01:27:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-06-03T01:27:27Z"
      lastUpdateTime: "2025-06-03T01:27:49Z"
      message: ReplicaSet "metrics-server-64fc948c75" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"local-path-provisioner","namespace":"local-path-storage"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"local-path-provisioner"}},"template":{"metadata":{"labels":{"app":"local-path-provisioner"}},"spec":{"containers":[{"command":["local-path-provisioner","--debug","start","--config","/etc/config/config.json"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"CONFIG_MOUNT_PATH","value":"/etc/config/"}],"image":"rancher/local-path-provisioner:v0.0.31","imagePullPolicy":"IfNotPresent","name":"local-path-provisioner","volumeMounts":[{"mountPath":"/etc/config/","name":"config-volume"}]}],"serviceAccountName":"local-path-provisioner-service-account","volumes":[{"configMap":{"name":"local-path-config"},"name":"config-volume"}]}}}}
    creationTimestamp: "2025-06-01T02:06:01Z"
    generation: 1
    name: local-path-provisioner
    namespace: local-path-storage
    resourceVersion: "702693"
    uid: ec01776c-d7cd-4967-a909-c8dd18b89e2f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CONFIG_MOUNT_PATH
            value: /etc/config/
          image: rancher/local-path-provisioner:v0.0.31
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-06-01T02:06:11Z"
      lastUpdateTime: "2025-06-01T02:06:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-06-01T02:06:01Z"
      lastUpdateTime: "2025-06-01T02:06:11Z"
      message: ReplicaSet "local-path-provisioner-74f9666bc9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"metallb","component":"controller"},"name":"controller","namespace":"metallb-system"},"spec":{"revisionHistoryLimit":3,"selector":{"matchLabels":{"app":"metallb","component":"controller"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"7472","prometheus.io/scrape":"true"},"labels":{"app":"metallb","component":"controller"}},"spec":{"containers":[{"args":["--port=7472","--log-level=info","--tls-min-version=VersionTLS12"],"env":[{"name":"METALLB_ML_SECRET_NAME","value":"memberlist"},{"name":"METALLB_DEPLOYMENT","value":"controller"}],"image":"quay.io/metallb/controller:v0.14.9","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/metrics","port":"monitoring"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"name":"controller","ports":[{"containerPort":7472,"name":"monitoring"},{"containerPort":9443,"name":"webhook-server","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/metrics","port":"monitoring"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/tmp/k8s-webhook-server/serving-certs","name":"cert","readOnly":true}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"securityContext":{"fsGroup":65534,"runAsNonRoot":true,"runAsUser":65534},"serviceAccountName":"controller","terminationGracePeriodSeconds":0,"volumes":[{"name":"cert","secret":{"defaultMode":420,"secretName":"metallb-webhook-cert"}}]}}}}
    creationTimestamp: "2025-05-08T00:51:23Z"
    generation: 1
    labels:
      app: metallb
      component: controller
    name: controller
    namespace: metallb-system
    resourceVersion: "118041"
    uid: a24d30ed-5e67-4dca-891f-87343ed5bbe9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app: metallb
        component: controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "7472"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: metallb
          component: controller
      spec:
        containers:
        - args:
          - --port=7472
          - --log-level=info
          - --tls-min-version=VersionTLS12
          env:
          - name: METALLB_ML_SECRET_NAME
            value: memberlist
          - name: METALLB_DEPLOYMENT
            value: controller
          image: quay.io/metallb/controller:v0.14.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          - containerPort: 9443
            name: webhook-server
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/k8s-webhook-server/serving-certs
            name: cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: controller
        serviceAccountName: controller
        terminationGracePeriodSeconds: 0
        volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: metallb-webhook-cert
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-05-08T00:51:23Z"
      lastUpdateTime: "2025-05-08T00:51:41Z"
      message: ReplicaSet "controller-bb5f47665" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-05-12T01:51:15Z"
      lastUpdateTime: "2025-05-12T01:51:15Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: black
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-03T01:15:34Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: black
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-blackbox-exporter
      app.kubernetes.io/version: v0.26.0
      helm.sh/chart: prometheus-blackbox-exporter-9.8.0
    name: black-prometheus-blackbox-exporter
    namespace: monitoring
    resourceVersion: "753824"
    uid: c793b988-a839-4284-82df-30c55dc5db4a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: black
        app.kubernetes.io/name: prometheus-blackbox-exporter
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 238debadbd6fed461c4b2bf4d7416e00d36791485f010dbb793cb2cbd7d888ed
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: black
          app.kubernetes.io/name: prometheus-blackbox-exporter
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --config.file=/config/blackbox.yaml
          image: quay.io/prometheus/blackbox-exporter:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: blackbox-exporter
          ports:
          - containerPort: 9115
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: black-prometheus-blackbox-exporter
        serviceAccountName: black-prometheus-blackbox-exporter
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: black-prometheus-blackbox-exporter
          name: config
  status:
    conditions:
    - lastTransitionTime: "2025-06-03T01:15:34Z"
      lastUpdateTime: "2025-06-03T01:15:34Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    - lastTransitionTime: "2025-06-03T01:25:35Z"
      lastUpdateTime: "2025-06-03T01:25:35Z"
      message: ReplicaSet "black-prometheus-blackbox-exporter-54cfcb5d87" has timed
        out progressing.
      reason: ProgressDeadlineExceeded
      status: "False"
      type: Progressing
    observedGeneration: 1
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.0-security-01
      helm.sh/chart: grafana-9.2.1
    name: kube-prom-stack-grafana
    namespace: monitoring
    resourceVersion: "749210"
    uid: f1565645-2eb0-4f2a-8b9e-f2034a80ff63
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prom-stack
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.0.0-security-01
          helm.sh/chart: grafana-9.2.1
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prom-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prom-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prom-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prom-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prom-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prom-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.0.0-security-01
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prom-stack-grafana
        serviceAccountName: kube-prom-stack-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prom-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prom-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prom-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-06-01T04:43:35Z"
      lastUpdateTime: "2025-06-01T04:45:46Z"
      message: ReplicaSet "kube-prom-stack-grafana-64774f5954" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-06-03T00:53:46Z"
      lastUpdateTime: "2025-06-03T00:53:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-operator
    namespace: monitoring
    resourceVersion: "727155"
    uid: 8157fcaa-52ec-40b5-bfd6-cfcd32064550
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: kube-prom-stack
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 72.9.0
          chart: kube-prometheus-stack-72.9.0
          heritage: Helm
          release: kube-prom-stack
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prom-stack-kube-prome-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.38.0
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.82.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prom-stack-kube-prome-operator
        serviceAccountName: kube-prom-stack-kube-prome-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prom-stack-kube-prome-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-06-01T04:43:37Z"
      lastUpdateTime: "2025-06-01T04:43:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-06-01T04:43:35Z"
      lastUpdateTime: "2025-06-01T04:43:37Z"
      message: ReplicaSet "kube-prom-stack-kube-prome-operator-d45c777c4" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.33.2
      release: kube-prom-stack
    name: kube-prom-stack-kube-state-metrics
    namespace: monitoring
    resourceVersion: "727310"
    uid: af679fd8-2102-4c11-958c-1d019eecdf0a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prom-stack
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.15.0
          helm.sh/chart: kube-state-metrics-5.33.2
          release: kube-prom-stack
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prom-stack-kube-state-metrics
        serviceAccountName: kube-prom-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-06-01T04:43:48Z"
      lastUpdateTime: "2025-06-01T04:43:48Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-06-01T04:43:35Z"
      lastUpdateTime: "2025-06-01T04:43:48Z"
      message: ReplicaSet "kube-prom-stack-kube-state-metrics-766df6c6fd" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-05-08T00:46:45Z"
    generation: 1
    labels:
      k8s-app: calico-kube-controllers
      pod-template-hash: 79949b87d
    name: calico-kube-controllers-79949b87d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-kube-controllers
      uid: 6dee672f-b90e-42b5-8233-0168642d7b4b
    resourceVersion: "632139"
    uid: 3b7cf545-be1c-47d4-b2b4-a3c35bc6552e
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
        pod-template-hash: 79949b87d
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: calico-kube-controllers
          pod-template-hash: 79949b87d
        name: calico-kube-controllers
        namespace: kube-system
      spec:
        containers:
        - env:
          - name: ENABLED_CONTROLLERS
            value: node
          - name: DATASTORE_TYPE
            value: kubernetes
          image: docker.io/calico/kube-controllers:v3.29.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -l
            failureThreshold: 6
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: calico-kube-controllers
          readinessProbe:
            exec:
              command:
              - /usr/bin/check-status
              - -r
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: calico-kube-controllers
        serviceAccountName: calico-kube-controllers
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-05-29T01:11:33Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: ccd8f67bc
    name: coredns-ccd8f67bc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 48415390-8267-4330-8d0f-67b3877af255
    resourceVersion: "648726"
    uid: 64cb646b-000e-48cf-b84e-49b5f251071e
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: ccd8f67bc
    template:
      metadata:
        annotations:
          priorityClassName: system-cluster-critical
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: ccd8f67bc
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: coredns/coredns:1.12.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-05-29T01:11:41Z"
    generation: 1
    labels:
      k8s-app: hostpath-provisioner
      pod-template-hash: c778b7559
    name: hostpath-provisioner-c778b7559
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hostpath-provisioner
      uid: 09839664-f1e0-47f6-920b-dfa292304339
    resourceVersion: "742498"
    uid: 4c4e8190-a31f-49ba-9634-1c0d84ef850b
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hostpath-provisioner
        pod-template-hash: c778b7559
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hostpath-provisioner
          pod-template-hash: c778b7559
      spec:
        containers:
        - env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: PV_DIR
            value: /var/snap/microk8s/common/default-storage
          - name: BUSYBOX_IMAGE
            value: busybox:1.28.4
          image: cdkbot/hostpath-provisioner:1.5.0
          imagePullPolicy: IfNotPresent
          name: hostpath-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/snap/microk8s/common/default-storage
            name: pv-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: microk8s-hostpath
        serviceAccountName: microk8s-hostpath
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/snap/microk8s/common/default-storage
            type: ""
          name: pv-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-06-03T01:27:27Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 64fc948c75
    name: metrics-server-64fc948c75
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 74f3c997-a37d-46ac-a670-5a2e9b3133d1
    resourceVersion: "754182"
    uid: 9e17243f-86cc-4e14-a5a8-4bd9add8fa7d
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 64fc948c75
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 64fc948c75
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=4443
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --kubelet-insecure-tls
          image: registry.k8s.io/metrics-server/metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 4443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/arch: amd64
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-06-01T02:06:01Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 74f9666bc9
    name: local-path-provisioner-74f9666bc9
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: ec01776c-d7cd-4967-a909-c8dd18b89e2f
    resourceVersion: "702692"
    uid: 004e38c5-7926-451b-b708-0b06e128d3a8
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 74f9666bc9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 74f9666bc9
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CONFIG_MOUNT_PATH
            value: /etc/config/
          image: rancher/local-path-provisioner:v0.0.31
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-05-08T00:51:23Z"
    generation: 1
    labels:
      app: metallb
      component: controller
      pod-template-hash: bb5f47665
    name: controller-bb5f47665
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: controller
      uid: a24d30ed-5e67-4dca-891f-87343ed5bbe9
    resourceVersion: "118039"
    uid: 0f5d6733-981b-406a-813b-fd56785c0f49
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: metallb
        component: controller
        pod-template-hash: bb5f47665
    template:
      metadata:
        annotations:
          prometheus.io/port: "7472"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: metallb
          component: controller
          pod-template-hash: bb5f47665
      spec:
        containers:
        - args:
          - --port=7472
          - --log-level=info
          - --tls-min-version=VersionTLS12
          env:
          - name: METALLB_ML_SECRET_NAME
            value: memberlist
          - name: METALLB_DEPLOYMENT
            value: controller
          image: quay.io/metallb/controller:v0.14.9
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          - containerPort: 9443
            name: webhook-server
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /metrics
              port: monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/k8s-webhook-server/serving-certs
            name: cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: controller
        serviceAccountName: controller
        terminationGracePeriodSeconds: 0
        volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: metallb-webhook-cert
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: black
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-03T01:15:34Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: black
      app.kubernetes.io/name: prometheus-blackbox-exporter
      pod-template-hash: 54cfcb5d87
    name: black-prometheus-blackbox-exporter-54cfcb5d87
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: black-prometheus-blackbox-exporter
      uid: c793b988-a839-4284-82df-30c55dc5db4a
    resourceVersion: "752379"
    uid: 2d033ceb-d5d7-4de5-84d5-f1e4eb6b6e52
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: black
        app.kubernetes.io/name: prometheus-blackbox-exporter
        pod-template-hash: 54cfcb5d87
    template:
      metadata:
        annotations:
          checksum/config: 238debadbd6fed461c4b2bf4d7416e00d36791485f010dbb793cb2cbd7d888ed
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: black
          app.kubernetes.io/name: prometheus-blackbox-exporter
          pod-template-hash: 54cfcb5d87
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --config.file=/config/blackbox.yaml
          image: quay.io/prometheus/blackbox-exporter:v0.26.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: blackbox-exporter
          ports:
          - containerPort: 9115
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: black-prometheus-blackbox-exporter
        serviceAccountName: black-prometheus-blackbox-exporter
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: black-prometheus-blackbox-exporter
          name: config
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.0-security-01
      helm.sh/chart: grafana-9.2.1
      pod-template-hash: 64774f5954
    name: kube-prom-stack-grafana-64774f5954
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prom-stack-grafana
      uid: f1565645-2eb0-4f2a-8b9e-f2034a80ff63
    resourceVersion: "749206"
    uid: a867347b-76e6-41f9-ad9f-95d8fe12a7a9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prom-stack
        app.kubernetes.io/name: grafana
        pod-template-hash: 64774f5954
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.0.0-security-01
          helm.sh/chart: grafana-9.2.1
          pod-template-hash: 64774f5954
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prom-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prom-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prom-stack-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prom-stack-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: kube-prom-stack-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: kube-prom-stack-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.0.0-security-01
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: kube-prom-stack-grafana
        serviceAccountName: kube-prom-stack-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-prom-stack-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: kube-prom-stack-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: kube-prom-stack-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      pod-template-hash: d45c777c4
      release: kube-prom-stack
    name: kube-prom-stack-kube-prome-operator-d45c777c4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prom-stack-kube-prome-operator
      uid: 8157fcaa-52ec-40b5-bfd6-cfcd32064550
    resourceVersion: "727153"
    uid: e8f6aaaf-a733-4a45-843a-9dcbece2d54d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: d45c777c4
        release: kube-prom-stack
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 72.9.0
          chart: kube-prometheus-stack-72.9.0
          heritage: Helm
          pod-template-hash: d45c777c4
          release: kube-prom-stack
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/kube-prom-stack-kube-prome-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.38.0
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.82.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prom-stack-kube-prome-operator
        serviceAccountName: kube-prom-stack-kube-prome-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: kube-prom-stack-kube-prome-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:43:35Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.33.2
      pod-template-hash: 766df6c6fd
      release: kube-prom-stack
    name: kube-prom-stack-kube-state-metrics-766df6c6fd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-prom-stack-kube-state-metrics
      uid: af679fd8-2102-4c11-958c-1d019eecdf0a
    resourceVersion: "727309"
    uid: 12461a37-1e59-4314-ba78-ce257bd63d42
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prom-stack
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 766df6c6fd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: kube-prom-stack
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.15.0
          helm.sh/chart: kube-state-metrics-5.33.2
          pod-template-hash: 766df6c6fd
          release: kube-prom-stack
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prom-stack-kube-state-metrics
        serviceAccountName: kube-prom-stack-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "6895144192714233309"
    creationTimestamp: "2025-06-01T04:43:36Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      managed-by: prometheus-operator
      release: kube-prom-stack
    name: alertmanager-kube-prom-stack-kube-prome-alertmanager
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: kube-prom-stack-kube-prome-alertmanager
      uid: aa5f678e-225d-4355-8bad-cf5e17da7d8a
    resourceVersion: "727295"
    uid: 34cea21d-9880-4c74-8ae9-15a23ba2ee08
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: kube-prom-stack-kube-prome-alertmanager
        app.kubernetes.io/instance: kube-prom-stack-kube-prome-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: kube-prom-stack-kube-prome-alertmanager
          app.kubernetes.io/instance: kube-prom-stack-kube-prome-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.28.1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - alertmanager
                  - key: alertmanager
                    operator: In
                    values:
                    - kube-prom-stack-kube-prome-alertmanager
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://kube-prom-stack-kube-prome-alertmanager.monitoring:9093
          - --web.route-prefix=/
          - --cluster.label=monitoring/kube-prom-stack-kube-prome-alertmanager
          - --cluster.peer=alertmanager-kube-prom-stack-kube-prome-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-kube-prom-stack-kube-prome-alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
            name: cluster-tls-config
            readOnly: true
            subPath: cluster-tls-config.yaml
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prom-stack-kube-prome-alertmanager
        serviceAccountName: kube-prom-stack-kube-prome-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prom-stack-kube-prome-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-kube-prom-stack-kube-prome-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prom-stack-kube-prome-alertmanager-web-config
        - name: cluster-tls-config
          secret:
            defaultMode: 420
            secretName: alertmanager-kube-prom-stack-kube-prome-alertmanager-cluster-tls-config
        - emptyDir: {}
          name: alertmanager-kube-prom-stack-kube-prome-alertmanager-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-kube-prom-stack-kube-prome-alertmanager-76b844b954
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-kube-prom-stack-kube-prome-alertmanager-76b844b954
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:47:11Z"
    generation: 1
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki
    namespace: monitoring
    resourceVersion: "749060"
    uid: d8478209-2a9d-45a0-9736-a898b784695f
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: loki
        release: loki
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: 52f2160434f6afa7add4ee73677fad071033a893bbbc78055199fd85bef1af93
          prometheus.io/port: http-metrics
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: loki
          name: loki
          release: loki
      spec:
        affinity: {}
        containers:
        - args:
          - -config.file=/etc/loki/loki.yaml
          image: grafana/loki:2.9.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: memberlist-port
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki
            name: config
          - mountPath: /data
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 4800
        volumes:
        - emptyDir: {}
          name: tmp
        - name: config
          secret:
            defaultMode: 420
            secretName: loki
        - emptyDir: {}
          name: storage
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 2
    collisionCount: 0
    currentReplicas: 2
    currentRevision: loki-7465cf4474
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updateRevision: loki-7465cf4474
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prom-stack
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "8485200610359982416"
    creationTimestamp: "2025-06-01T04:43:37Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prom-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 72.9.0
      chart: kube-prometheus-stack-72.9.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: kube-prom-stack-kube-prome-prometheus
      operator.prometheus.io/shard: "0"
      release: kube-prom-stack
    name: prometheus-kube-prom-stack-kube-prome-prometheus
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: kube-prom-stack-kube-prome-prometheus
      uid: eddcdbe2-0008-4a82-870c-70c830add687
    resourceVersion: "727439"
    uid: 2c4c93e6-ba26-4d1a-b0a0-ec721d496d84
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: kube-prom-stack-kube-prome-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: kube-prom-stack-kube-prome-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: kube-prom-stack-kube-prome-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: kube-prom-stack-kube-prome-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 3.4.1
          operator.prometheus.io/name: kube-prom-stack-kube-prome-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: kube-prom-stack-kube-prome-prometheus
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - prometheus
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - kube-prom-stack-kube-prome-prometheus
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=http://kube-prom-stack-kube-prome-prometheus.monitoring:9090
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=10d
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v3.4.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 5
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: stack-pvc-prometheus
            subPath: prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
            name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
            name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.82.2
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
            name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: kube-prom-stack-kube-prome-prometheus
        serviceAccountName: kube-prom-stack-kube-prome-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prom-stack-kube-prome-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-kube-prom-stack-kube-prome-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
          name: prometheus-kube-prom-stack-kube-prome-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-kube-prom-stack-kube-prome-prometheus-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: stack-pvc-prometheus
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: stack-sc
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-kube-prom-stack-kube-prome-prometheus-6dcb9f95c
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-kube-prom-stack-kube-prome-prometheus-6dcb9f95c
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-06-01T04:49:24Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.7.1
      helm.sh/chart: tempo-1.21.1
    name: tempo
    namespace: monitoring
    resourceVersion: "748995"
    uid: e5637819-1489-4f6b-a7e5-27c1139df61f
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    serviceName: tempo-headless
    template:
      metadata:
        annotations:
          checksum/config: 6bd3b3f1438e05710dd9abcb76f71c2199bf38133349a5ce61e88a4b4b9471c8
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: tempo
          app.kubernetes.io/name: tempo
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/conf/tempo.yaml
          - -mem-ballast-size-mbs=1024
          image: grafana/tempo:2.7.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: tempo
          ports:
          - containerPort: 3100
            name: prom-metrics
            protocol: TCP
          - containerPort: 6831
            name: jaeger-thrift-c
            protocol: UDP
          - containerPort: 6832
            name: jaeger-thrift-b
            protocol: UDP
          - containerPort: 14268
            name: jaeger-thrift-h
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55680
            name: otlp-legacy
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 55681
            name: otlp-httplegacy
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 55678
            name: opencensus
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3100
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: tempo-conf
          - mountPath: /var/tempo
            name: storage
        - args:
          - -config
          - /conf/tempo-query.yaml
          image: grafana/tempo-query:2.7.1
          imagePullPolicy: IfNotPresent
          name: tempo-query
          ports:
          - containerPort: 16686
            name: jaeger-ui
            protocol: TCP
          - containerPort: 16687
            name: jaeger-metrics
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: tempo-query-conf
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: tempo
        serviceAccountName: tempo
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: tempo-query
          name: tempo-query-conf
        - configMap:
            defaultMode: 420
            name: tempo
          name: tempo-conf
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: stack-sc
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: tempo-584c8c8b9c
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: tempo-584c8c8b9c
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"CronJob","metadata":{"annotations":{},"name":"coleta-du","namespace":"kube-system"},"spec":{"jobTemplate":{"spec":{"template":{"spec":{"containers":[{"args":["apk add --no-cache coreutils;\ndu -sh /var/lib/docker/containers /var/lib/kubelet/pods /var/log /opt/applications || true\n"],"command":["/bin/sh","-c"],"image":"alpine:latest","name":"coleta-du"}],"restartPolicy":"Never"}}}},"schedule":"0 * * * *"}}
    creationTimestamp: "2025-06-03T01:41:36Z"
    generation: 1
    name: coleta-du
    namespace: kube-system
    resourceVersion: "758709"
    uid: f89c9ca6-4798-4912-b940-470501f7ed3b
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        template:
          metadata:
            creationTimestamp: null
          spec:
            containers:
            - args:
              - |
                apk add --no-cache coreutils;
                du -sh /var/lib/docker/containers /var/lib/kubelet/pods /var/log /opt/applications || true
              command:
              - /bin/sh
              - -c
              image: alpine:latest
              imagePullPolicy: Always
              name: coleta-du
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            restartPolicy: Never
            schedulerName: default-scheduler
            securityContext: {}
            terminationGracePeriodSeconds: 30
    schedule: 0 * * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-06-03T02:00:00Z"
    lastSuccessfulTime: "2025-06-03T02:00:19Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-06-03T02:00:00Z"
    creationTimestamp: "2025-06-03T02:00:00Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
      batch.kubernetes.io/job-name: coleta-du-29148600
      controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
      job-name: coleta-du-29148600
    name: coleta-du-29148600
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: coleta-du
      uid: f89c9ca6-4798-4912-b940-470501f7ed3b
    resourceVersion: "758706"
    uid: af8c484d-b282-4d24-be81-4f7b97e124ef
  spec:
    backoffLimit: 6
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
          batch.kubernetes.io/job-name: coleta-du-29148600
          controller-uid: af8c484d-b282-4d24-be81-4f7b97e124ef
          job-name: coleta-du-29148600
      spec:
        containers:
        - args:
          - |
            apk add --no-cache coreutils;
            du -sh /var/lib/docker/containers /var/lib/kubelet/pods /var/log /opt/applications || true
          command:
          - /bin/sh
          - -c
          image: alpine:latest
          imagePullPolicy: Always
          name: coleta-du
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2025-06-03T02:00:19Z"
    conditions:
    - lastProbeTime: "2025-06-03T02:00:19Z"
      lastTransitionTime: "2025-06-03T02:00:19Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-06-03T02:00:19Z"
      lastTransitionTime: "2025-06-03T02:00:19Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-06-03T02:00:00Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
